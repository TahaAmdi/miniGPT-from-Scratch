{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd501dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d65e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q heads:\n",
      " [[[1. 2.]\n",
      "  [5. 6.]]\n",
      "\n",
      " [[3. 4.]\n",
      "  [7. 8.]]] \n",
      "\n",
      "Head 1 — scores:\n",
      "[[ 5. 17.]\n",
      " [17. 61.]]\n",
      "Head 1 — softmax probs:\n",
      "[[0.00001 0.99999]\n",
      " [0.      1.     ]]\n",
      "Head 1 — output:\n",
      "[[4.99998 5.99998]\n",
      " [5.      6.     ]]\n",
      "\n",
      "Head 2 — scores:\n",
      "[[ 25.  53.]\n",
      " [ 53. 113.]]\n",
      "Head 2 — softmax probs:\n",
      "[[0. 1.]\n",
      " [0. 1.]]\n",
      "Head 2 — output:\n",
      "[[7. 8.]\n",
      " [7. 8.]]\n",
      "\n",
      "Concatenated heads:\n",
      " [[4.99998 5.99998 7.      8.     ]\n",
      " [5.      6.      7.      8.     ]] \n",
      "\n",
      "Final output:\n",
      " [[4.99998 5.99998 7.      8.     ]\n",
      " [5.      6.      7.      8.     ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"-----------------------------From Scratch Example----------------------------- \"\"\"\n",
    "\n",
    "# ---------- helper ----------\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"نسخه پایدار عددیِ softmax.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "# ---------- 1) ورودیِ مثال ----------\n",
    "x = np.array([\n",
    "    [1., 2., 3., 4.],  # توکن ۱\n",
    "    [5., 6., 7., 8.]   # توکن ۲\n",
    "])\n",
    "\n",
    "# ---------- 2) Q, K, V (لایه‌های خطی = ماتریس همانی) ----------\n",
    "W_q = W_k = W_v = np.eye(4)\n",
    "\n",
    "Q = x @ W_q.T\n",
    "K = x @ W_k.T\n",
    "V = x @ W_v.T\n",
    "\n",
    "# ---------- 3) تقسیم به ۲ هدِ سایز ۲ ----------\n",
    "num_heads = 2\n",
    "head_dim   = 2\n",
    "seq_len    = x.shape[0]   # 2\n",
    "d_model    = x.shape[1]   # 4\n",
    "\n",
    "# تغییر شکل به (num_heads, seq_len, head_dim)\n",
    "Q_heads = Q.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "K_heads = K.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "V_heads = V.reshape(seq_len, num_heads, head_dim).transpose(1, 0, 2)\n",
    "\n",
    "print(\"Q heads:\\n\", Q_heads, \"\\n\")\n",
    "\n",
    "# ---------- 4-5) محاسبه attention برای هر هد ----------\n",
    "head_outputs = []\n",
    "for h in range(num_heads):\n",
    "    Q_h, K_h, V_h = Q_heads[h], K_heads[h], V_heads[h]\n",
    "\n",
    "    # نمره‌ها = Q_h @ K_hᵀ   (برای سادگی بدون scaling)\n",
    "    scores = Q_h @ K_h.T\n",
    "    probs  = softmax(scores, axis=-1)\n",
    "    out_h  = probs @ V_h\n",
    "\n",
    "    print(f\"Head {h+1} — scores:\\n{scores}\")\n",
    "    print(f\"Head {h+1} — softmax probs:\\n{probs}\")\n",
    "    print(f\"Head {h+1} — output:\\n{out_h}\\n\")\n",
    "\n",
    "    head_outputs.append(out_h)\n",
    "\n",
    "head_outputs = np.stack(head_outputs)  # (num_heads, seq_len, head_dim)\n",
    "\n",
    "# ---------- 6) چسباندن هدها ----------\n",
    "concat = head_outputs.transpose(1, 0, 2).reshape(seq_len, d_model)\n",
    "print(\"Concatenated heads:\\n\", concat, \"\\n\")\n",
    "\n",
    "# ---------- 7) لایه out_proj (همانی) ----------\n",
    "W_o = np.eye(d_model)  # در عمل trainable است\n",
    "final_output = concat @ W_o.T\n",
    "\n",
    "print(\"Final output:\\n\", final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b19afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "Input tensor:\n",
      " tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13)\n",
    "print(\"Input tensor:\\n\", x)\n",
    "\n",
    "x = torch.arange(1, 13, dtype=torch.float32).view(3, 4)  # (3, 4    # tokens, 4 features)\n",
    "print(\"Input tensor:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f244b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor with large value:\n",
      " tensor([[   1.,    2.,    3.,    4.],\n",
      "        [   5.,    6.,    7.,    8.],\n",
      "        [   9.,   10.,   11., 1000.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13, dtype=torch.float32).view(3, 4)  # (3, 4    # tokens, 4 features)\n",
    "\n",
    "x[-1, -1] = 1000 # (3, 4    # tokens, 4 features)\n",
    "print(\"Input tensor with large value:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840410e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor reshaped to (2, 2, 3):\n",
      " tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]]])\n",
      "Input tensor reshaped to (6, 2):\n",
      " tensor([[ 1.,  2.],\n",
      "        [ 3.,  4.],\n",
      "        [ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.],\n",
      "        [11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13, dtype=torch.float32).view(2, 2, 3)  # (3, 4    # tokens, 4 features)\n",
    "print(\"Input tensor reshaped to (2, 2, 3):\\n\", x)\n",
    "\n",
    "x = torch.arange(1, 13, dtype=torch.float32).view(6, 2) # (6, 2    # tokens, 2 features)\n",
    "print(\"Input tensor reshaped to (6, 2):\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dedf2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor reshaped to (2, 2, 3):\n",
      " tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]]])\n",
      "tensor([[[ 1.,  4.],\n",
      "         [ 2.,  5.],\n",
      "         [ 3.,  6.]],\n",
      "\n",
      "        [[ 7., 10.],\n",
      "         [ 8., 11.],\n",
      "         [ 9., 12.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13, dtype=torch.float32).view(2, 2, 3)  # (3, 4    # tokens, 4 features)\n",
    "print(\"Input tensor reshaped to (2, 2, 3):\\n\", x)\n",
    "\n",
    "x = x.transpose(1, 2)  # (2, 3, 2) --تبدیل به (batch, num_heads, tokens, head_dim)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd2f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor with shape (6, 2):\n",
      " tensor([[ 1,  2],\n",
      "        [ 3,  4],\n",
      "        [ 5,  6],\n",
      "        [ 7,  8],\n",
      "        [ 9, 10],\n",
      "        [11, 12]])\n",
      "Strides of the tensor: (2, 1)\n",
      "Shape of the tensor: torch.Size([6, 2])\n",
      "Number of dimensions: 2\n",
      "Number of elements: 12\n",
      "Data type of the tensor: torch.int64\n",
      "Device of the tensor: cpu\n",
      "Is the tensor contiguous? True\n",
      "\n",
      "--- Simple Attention Mechanism ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,13).view(6,2)\n",
    "print(\"Input tensor with shape (6, 2):\\n\", x)\n",
    "\n",
    "x_1 = x.stride()  # Get the strides of the tensor\n",
    "print(\"Strides of the tensor:\", x_1)\n",
    "\n",
    "\n",
    "print(\"Shape of the tensor:\", x.shape)\n",
    "print(\"Number of dimensions:\", x.dim())\n",
    "print(\"Number of elements:\", x.numel())\n",
    "print(\"Data type of the tensor:\", x.dtype)\n",
    "print(\"Device of the tensor:\", x.device)\n",
    "print(\"Is the tensor contiguous?\", x.is_contiguous())\n",
    "# Define a simple attention mechanism\n",
    "print(\"\\n--- Simple Attention Mechanism ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b64f1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "Strides of the tensor: (6, 3, 1)\n",
      "Transposed tensor:\n",
      " tensor([[[ 0,  6],\n",
      "         [ 3,  9]],\n",
      "\n",
      "        [[ 1,  7],\n",
      "         [ 4, 10]],\n",
      "\n",
      "        [[ 2,  8],\n",
      "         [ 5, 11]]])\n",
      "Shape of the transposed tensor: torch.Size([3, 2, 2])\n",
      "Strides of the transposed tensor: (1, 3, 6)\n",
      "Shape of the transposed tensor: torch.Size([3, 2, 2])\n",
      "Strides of the transposed tensor: (1, 3, 6)\n",
      "\n",
      "--- Simple Attention Mechanism ---\n",
      "\n",
      "Storage of the transposed tensor:  0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taha\\AppData\\Local\\Temp\\ipykernel_2520\\3441985764.py:18: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  y.storage()\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,12).view(2,2,3)\n",
    "print(\"Input tensor:\\n\", x)\n",
    "\n",
    "x_2 = x.stride()  # Get the strides of the tensor\n",
    "print(\"Strides of the tensor:\", x_2)\n",
    "\n",
    "y = x.transpose(0,2)\n",
    "print(\"Transposed tensor:\\n\", y)\n",
    "# Check the shape and strides of the transposed tensor\n",
    "print(\"Shape of the transposed tensor:\", y.shape)\n",
    "print(\"Strides of the transposed tensor:\", y.stride())\n",
    "# Check the shape and strides of the transposed tensor\n",
    "print(\"Shape of the transposed tensor:\", y.shape)\n",
    "print(\"Strides of the transposed tensor:\", y.stride())\n",
    "# Define a simple attention mechanism\n",
    "print(\"\\n--- Simple Attention Mechanism ---\\n\")\n",
    "\n",
    "y.storage()\n",
    "# Check the storage of the transposed tensor\n",
    "print(\"Storage of the transposed tensor:\", y.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaa03e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is It contiguous? True\n",
      "Is It contiguous? False\n",
      "torch.Size([6, 2])\n",
      "(1, 6)\n",
      "Storage of the transposed tensor:  0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      " 7\n",
      " 8\n",
      " 9\n",
      " 10\n",
      " 11\n",
      "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 12]\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,12).view(2,6)\n",
    "# Print the input tensor\n",
    "print(\"Is It contiguous?\",x.is_contiguous())\n",
    "\n",
    "y = x.transpose(0, 1)\n",
    "# Print the transposed tensor\n",
    "print(\"Is It contiguous?\",y.is_contiguous())\n",
    "\n",
    "# Check the storage of the transposed tensor\n",
    "print(y.shape)\n",
    "#stride shows the step size to move to the next element in each dimension.\n",
    "print(y.stride())\n",
    "#storage shows the underlying data buffer and shows how the data is stored in memory.\n",
    "print(\"Storage of the transposed tensor:\", y.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1344466c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped tensor:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Modified tensor:\n",
      " tensor([[100,   2,   3],\n",
      "        [  4,   5,   6],\n",
      "        [  7,   8,   9],\n",
      "        [ 10,  11,  12]])\n",
      "Original tensor after modification:\n",
      " tensor([100,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,13)\n",
    "y = x.reshape(4,3)\n",
    "# Print the reshaped tensor\n",
    "print(\"Reshaped tensor:\\n\", y)\n",
    "\n",
    "y[0,0] = 100\n",
    "# Print the modified tensor\n",
    "print(\"Modified tensor:\\n\", y)\n",
    "# Summary\n",
    "print(\"Original tensor after modification:\\n\", x)\n",
    "# A ‘View’ uses the same chunk of memory block as the original tensor, and thus any changes among this memory chunk will affect all the views and the original tensor that’s associated with it.</br>\n",
    "# A ‘View’ can be contiguous or non-contiguous.</br>\n",
    "# A non-contiguous tensor view can be converted to a contiguous one, and it would make a copy of it, so the data will not be associated with the original data chunk anymore.</br>\n",
    "# Stride Position Formula: Given a strides (A, B, C), the position of\n",
    "# the index (j, k, v) in the 1D data array is (A *j + B*k + C*v)</br>\n",
    "# Difference between view() and reshape():</br>\n",
    "# view() cannot apply on ‘non-contiguous’ tensor /view. It returns a view.\n",
    "# reshape() can apply on both ‘contiguous’ and ‘non-contiguous’ tensor/view\n",
    "# When possible, it will return a view; When the data is non-contiguous, it makes a new copy of it.\n",
    "# Reshape() is more flexible than view() but may involve copying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec3fcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor:\n",
      " tensor([[ 1.3911,  1.5115,  0.7717],\n",
      "        [-0.5112,  0.2792, -0.5493]])\n",
      "Transposed tensor:\n",
      " tensor([[ 1.3911, -0.5112],\n",
      "        [ 1.5115,  0.2792],\n",
      "        [ 0.7717, -0.5493]])\n",
      "Is the transposed tensor contiguous? False\n",
      "Reshaped tensor:\n",
      " tensor([[ 1.3911, -0.5112,  1.5115],\n",
      "        [ 0.2792,  0.7717, -0.5493]])\n"
     ]
    }
   ],
   "source": [
    "x = torch. randn(2, 3)\n",
    "# Print the random tensor\n",
    "print(\"Random tensor:\\n\", x)\n",
    "\n",
    "y = x.transpose(1, 0)\n",
    "# Print the transposed tensor\n",
    "print(\"Transposed tensor:\\n\", y)\n",
    "# Check if the transposed tensor is contiguous\n",
    "print(\"Is the transposed tensor contiguous?\", y.is_contiguous())\n",
    "\n",
    "# Reshape the transposed tensor to a new shape\n",
    "# Note: The shape must be compatible with the number of elements in the tensor\n",
    "#when we know tensor is contiguous, we can use view() to reshape it\n",
    "# If the tensor is not contiguous, we can use contiguous() to make it contiguous first\n",
    "# we also can use reshape() to reshape it\n",
    "# but reshape() may create a copy if the tensor is not contiguous\n",
    "# In this case, we will use contiguous() to ensure the tensor is contiguous before reshaping\n",
    "z= y.contiguous().view(2,3)\n",
    "# Print the reshaped tensor\n",
    "print(\"Reshaped tensor:\\n\", z)\n",
    "#transpose() does not change the original tensor, it returns a new tensor with the specified dimensions swapped\n",
    "# The original tensor remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0990f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([[[-0.3301,  0.8041, -0.6608,  2.1843, -0.3718, -1.2365],\n",
      "         [-1.7740,  1.9076,  0.6946, -1.1267,  1.2110, -1.3329],\n",
      "         [ 0.6670,  0.8639, -0.3981, -1.0714, -0.5151, -1.9603],\n",
      "         [ 0.6216,  0.7644, -0.7178,  0.5979,  0.2320, -0.3983]]]) \n",
      "\n",
      "Reshaped tensor:\n",
      " tensor([[[[-0.3301,  0.8041],\n",
      "          [-0.6608,  2.1843],\n",
      "          [-0.3718, -1.2365]],\n",
      "\n",
      "         [[-1.7740,  1.9076],\n",
      "          [ 0.6946, -1.1267],\n",
      "          [ 1.2110, -1.3329]],\n",
      "\n",
      "         [[ 0.6670,  0.8639],\n",
      "          [-0.3981, -1.0714],\n",
      "          [-0.5151, -1.9603]],\n",
      "\n",
      "         [[ 0.6216,  0.7644],\n",
      "          [-0.7178,  0.5979],\n",
      "          [ 0.2320, -0.3983]]]])\n",
      "Reshaped tensor:\n",
      " tensor([[[[-0.3301,  0.8041, -0.6608],\n",
      "          [ 2.1843, -0.3718, -1.2365]],\n",
      "\n",
      "         [[-1.7740,  1.9076,  0.6946],\n",
      "          [-1.1267,  1.2110, -1.3329]],\n",
      "\n",
      "         [[ 0.6670,  0.8639, -0.3981],\n",
      "          [-1.0714, -0.5151, -1.9603]],\n",
      "\n",
      "         [[ 0.6216,  0.7644, -0.7178],\n",
      "          [ 0.5979,  0.2320, -0.3983]]]])\n"
     ]
    }
   ],
   "source": [
    "x= torch.randn(1, 4, 6)  # Example input tensor with shape (batch_size=1, token=2, d_in=4\n",
    "batch, num_token, dim_in = x.shape  # Example shape (batch_size=1, token=2, d_in=4)\n",
    "\n",
    "# Print the input tensor\n",
    "print(\"Input tensor:\\n\", x, \"\\n\")\n",
    "\n",
    "num_heads = 3\n",
    "head_dim   = 2\n",
    "\n",
    "# Reshape the input tensor to (batch_size, num_heads, num_tokens, head_dim)\n",
    "# This is necessary for multi-head attention to work correctly\n",
    "x = x.view(batch, num_token, num_heads, head_dim)\n",
    "# Print the reshaped tensor\n",
    "print(\"Reshaped tensor:\\n\", x)\n",
    "\n",
    "num_heads = 2\n",
    "head_dim   = 3\n",
    "\n",
    "# Reshape the input tensor to (batch_size, num_heads, num_tokens, head_dim)\n",
    "# This is necessary for multi-head attention to work correctly\n",
    "x = x.view(batch, num_token, num_heads, head_dim)\n",
    "# Print the reshaped tensor\n",
    "print(\"Reshaped tensor:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b25af56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor:\n",
      " tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #1\n",
    " [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    " [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    " [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "\n",
    "# Print the input tensor\n",
    "# Transpose the last two dimensions of the tensor\n",
    "# This is useful for operations like matrix multiplication or attention mechanisms  \n",
    "# where we need to swap the dimensions for correct alignment\n",
    "y = a.transpose(2, 3)  # Transpose the last two dimensions\n",
    "\n",
    "# Print the transposed tensor\n",
    "print(\"Transposed tensor:\\n\", a @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85bd0537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head of the tensor:\n",
      " tensor([[0.2745, 0.6584, 0.2775, 0.8573],\n",
      "        [0.8993, 0.0390, 0.9268, 0.7388],\n",
      "        [0.7179, 0.7058, 0.9156, 0.4340]])\n",
      "Second head of the tensor:\n",
      " tensor([[0.0772, 0.3565, 0.1479, 0.5331],\n",
      "        [0.4066, 0.2318, 0.4545, 0.9737],\n",
      "        [0.4606, 0.5159, 0.4220, 0.5786]])\n"
     ]
    }
   ],
   "source": [
    "first_head = a[0, 0, :, :]\n",
    "# Print the first head of the tensor\n",
    "print(\"First head of the tensor:\\n\", first_head)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "# Print the second head of the tensor\n",
    "print(\"Second head of the tensor:\\n\", second_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa1783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "Reshaped tensor:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Tensor after unsqueeze:\n",
      " tensor([[[ 1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8],\n",
      "         [ 9, 10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 13)\n",
    "print(\"Input tensor:\\n\", x)\n",
    "\n",
    "x = x.view(3, 4)  # Reshape to (3, 4)\n",
    "print(\"Reshaped tensor:\\n\", x)\n",
    "\n",
    "x = x.unsqueeze(0)  # Remove the first dimension\n",
    "print(\"Tensor after unsqueeze:\\n\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd2dc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "Reshaped tensor:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Input tensor:\n",
      " torch.Size([12])\n",
      "Tensor after unsqueeze:\n",
      " torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(\"Input tensor:\\n\", x)\n",
    "\n",
    "x= x.view(3, 4)  # Reshape to (3, 4)\n",
    "print(\"Reshaped tensor:\\n\", x)\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(\"Input tensor:\\n\", x.shape)\n",
    "x = x.unsqueeze(0) # add a new dimension at the start\n",
    "print(\"Tensor after unsqueeze:\\n\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff90eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor:\n",
      " tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "Tensor after unsqueeze:\n",
      " tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "print(\"Input tensor:\\n\", x)\n",
    "x = x.unsqueeze(0).expand(-1, -1) # unsqueeze add a new dimension at the start and expand it to match the shape\n",
    "# Print the tensor after unsqueeze\n",
    "print(\"Tensor after unsqueeze:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2a98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    " \"vocab_size\": 50257, # Vocabulary size\n",
    " \"context_length\": 1024, # Context length\n",
    " \"emb_dim\": 768, # Embedding dimension\n",
    " \"n_heads\": 12, # Number of attention heads\n",
    " \"n_layers\": 12, # Number of layers\n",
    " \"drop_rate\": 0.1, # Dropout rate\n",
    " \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55965eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential( #1\n",
    "        *[DummyTransformerBlock(cfg) #1\n",
    "        for _ in range(cfg[\"n_layers\"])] #1\n",
    "        ) #1\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]) #2\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module): #3\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x): #4\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module): #5\n",
    "    def __init__(self, normalized_shape, eps=1e-5): #6\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb4f0379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "753f3938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d29542f",
   "metadata": {},
   "source": [
    "## روند آموزش یک Language Model (به‌سبک GPT، ساده‌شده)\n",
    "\n",
    "| گام | توضیح | شکل (ابعاد نمونه‌ای) |\n",
    "|-----|-------|-----------------------|\n",
    "| 1. ساخت واژگان (Vocabulary) | تمام متن توکنایز می‌شود؛‌ واژگان با اندازهٔ V (مثلاً 50k) شکل می‌گیرد و برای هر توکن یک بردار در ماتریس یادگیری‌پذیرِ embedding ساخته می‌شود. | E ∈ ℝ^{V × d_model} |\n",
    "| 2. تبدیل متن به ID | متن → دنبالهٔ بلندی از اعداد: 17, 328, 5, 42,… | بردار ۱بعدی خیلی بلند |\n",
    "| 3. بُرش به بلوک‌های طول ثابت | با context_length = 32 دنباله را به قطعات 32تایی می‌بُریم: <br>`[t0…t31]`, [t32…t63], … | اگر 1024 توکن داشته باشیم → 32 بلوک <br>`input.shape = (32, 32)` |\n",
    "| 4. ساخت ورودی/هدف (shift-one) | برای هر بلوک: <br>`inputs = [t0…t30]`   <br>`targets = [t1…t31]`  | inputs, targets ∈ ℤ^{batch, seq_len} |\n",
    "| 5. Embedding + Position | x = E[inputs] + P  ⇒ شکل (batch, seq_len, d_model) |\n",
    "| 6. عبور از لایه‌های ترنسفورمر | خروجی با همان شکل برمی‌گردد. |\n",
    "| 7. نگاشت به فضای واژگان | logits = x @ Wᵀ + b  با ابعاد (batch, seq_len, V) |\n",
    "| 8. محاسبۀ Loss | Cross-Entropy بین logits و targets. |\n",
    "| 9. به‌روزرسانی وزن‌ها | Optimizer تمام پارامترهای یادگیری‌پذیر (`E`, ترنسفورمرها, W, `b`) را در مرحلهٔ backward آپدیت می‌کند. |\n",
    "\n",
    "---\n",
    "\n",
    "### نکته‌های کلیدی\n",
    "\n",
    "* **Embedding Matrix فقط به V × d_model وابسته است**؛ اندازهٔ کتاب‌ها روی سایز آن اثر ندارد، بلکه روی تعداد batch‌ها اثر می‌گذارد.  \n",
    "* Shift-one باعث می‌شود مدل در هر موقعیت، توکن بعدی را پیش‌بینی کند.  \n",
    "* پارامترها یک‌بار در __init__ ساخته می‌شوند؛ در هر گام آموزش طی backward به‌روزرسانی می‌شوند و در فراخوانی‌های بعدی forward خودبه‌خود مقدار جدید دارند.  \n",
    "* در صورت نیاز به دادهٔ بیشتر می‌توان stride را کوچک‌تر از context_length گرفت (پنجره‌های هم‌پوشان).\n",
    "\n",
    "> «یک زبان-مدل، متن را به قطعات طول ثابت می‌بُرد، ورودی و خروجیِ شیفت‌شده می‌سازد، از شبکه عبور می‌دهد، روی logits softmax + cross-entropy می‌زند و وزن‌های embedding و ترنسفورمر را با backprop یاد می‌گیرد.»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447bd51",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">2.1 Tokenization</h1>\n",
    "<span style=\"color:rgb(255, 255, 255);\">-----------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5497b0e",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(253, 181, 229);\">* Downlaod Dataset from github:</span>\n",
    "1.</br>import urllib.request library\n",
    "</br>\n",
    "</br>2. Go to Github\n",
    "</br>\n",
    "</br>3. Select Raw\n",
    "</br>\n",
    "</br>4. Copy Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ed24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Compeleted\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "filename = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Download Compeleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca2463",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(253, 181, 229);\">* Open and Read the-verdict.txt</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a811698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    " raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebcd81",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">2.2 Goal:</h1>\n",
    "Our goal is to <span style=\"color:rgb(245, 142, 150);\">tokenize</span> this 20,479-character short story into</br>\n",
    "<span style=\"color:rgb(245, 142, 150);\">individual words and special characters</span> that we can then</br>\n",
    "turn into <span style=\"color:rgb(245, 142, 150);\">embeddings</span>  for LLM training.\n",
    "</br> --------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b970f5f",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(253, 181, 229);\">* Simple split e tokenization scheme model:</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799749f",
   "metadata": {},
   "source": [
    "<span style=\"color:rgba(249, 244, 248, 0.83);\">somewords are still connected to punctuation characters that</br>\n",
    "we want to have as separate list entries and make all lowercase\n",
    "</br> and delete whitespace ---> r.split(r'([:;.,?!{}[]... -- _ __]|\\\\s)',text)\n",
    "</br></br>it means:\n",
    "1. r'()' -> for recognize ponctuation in a text and \n",
    "</br>2.|\\\\s for ignore whitespace\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c5163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' world', '.', ' This', ',', ' is a test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result2 = re.split(r'([,.]|\\\\s)', text)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a9686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is this', '--', 'a test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22328bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 4578\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '-', '-', 'though', 'a', 'good', 'fellow', 'enough', '-', '-', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|[.,:;!?\\\"()\\[\\]—–-]\", raw_text)\n",
    "\n",
    "print(\"Total number of character:\", len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6c3a7",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">2.3 Converting tokens into token IDs\n",
    "</h1>\n",
    "<span style=\"color:rgb(255, 255, 255);\">-----------------------------------------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446668d7",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(245, 142, 150);\">convert tokens from a python string to an integer to produce the token IDs:</span></br>\n",
    "</br>2. build our vocabulary first</br>\n",
    "</br>3. mapping each unique word with this vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3642512",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(252, 126, 42); font-size:20px\">2.3.1 sort them alphabetically\n",
    "to determine the vocabulary size: \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14a696b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155\n",
      "['!', '\"', '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', \"Don't\", 'Dubarry_', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', \"Gisburn's\", 'Gisburns', 'Grafton', 'Greek', 'Grindle', \"Grindle's\", 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', \"Hermia's\", 'His', 'How', 'I', \"I'd\", \"I'll\", \"I've\", 'If', 'In', 'It', \"It's\", 'Jack', \"Jack's\", 'Jove', 'Just', 'Lord', 'Made', 'Miss', \"Money's\", 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', \"She's\", 'Stroud', \"Stroud's\", 'Strouds', 'Suddenly', 'That', \"That's\", 'The', 'Then', 'There', 'They', 'This', 'Those', 'Though', 'Thwing', \"Thwing's\", 'Thwings', 'To', 'Usually', 'Venetian', 'Victor', 'Was', 'We', 'Well', 'What', 'When', 'Why', 'Yes', 'You', '_I', '_am_', '_famille-verte_', '_felt_', '_has_', '_have_', '_jardiniere_', '_mine_', '_not_', '_rose', '_rs_', '_that_', '_the_', '_was_', '_were_', 'a', 'abdication', 'able', 'about', 'above', 'abruptly', 'absolute', 'absorbed', 'absurdity', 'academic', 'accuse', 'accustomed', 'across', 'activity', 'add', 'added', 'admirers', 'adopted', 'adulation', 'advance', 'aesthetic', 'affect', 'afraid', 'after', 'afterward', 'again', 'ago', 'ah', 'air', 'alive', 'all', 'almost', 'alone', 'along', 'always', 'amazement', 'amid', 'among', 'amplest', 'amusing', 'an', 'and', 'another', 'answer', 'answered', 'any', 'anything', 'anywhere', 'apparent', 'apparently', 'appearance', 'appeared', 'appointed', 'are', 'arm', 'arm-chair', 'arm-chairs', 'arms', 'art', 'articles', 'artist', 'as', 'aside', 'asked', 'at', 'atmosphere', 'atom', 'attack', 'attention', 'attitude', 'audacities', 'away', 'awful', 'axioms', 'azaleas', 'back', 'background', 'balance', 'balancing', 'balustraded', 'basking', 'bath-rooms', 'be', 'beaming', 'bean-stalk', 'bear', 'beard', 'beauty', 'became', 'because', 'becoming', 'bed', 'been', 'before', 'began', 'begun', 'behind', 'being', 'believed', 'beneath', 'bespoke', 'better', 'between', 'big', 'bits', 'bitterness', 'blocked', 'born', 'borne', 'boudoir', 'bravura', 'break', 'breaking', 'breathing', 'bric-a-brac', 'briefly', 'brings', 'bronzes', 'brought', 'brown', 'brush', 'bull', 'business', 'but', 'buying', 'by', 'called', 'came', 'can', 'canvas', 'canvases', 'cards', 'care', 'career', 'caught', 'central', 'chair', 'chap', 'characteristic', 'charming', 'cheap', 'check', 'cheeks', 'chest', 'chimney-piece', 'chucked', 'cigar', 'cigarette', 'cigars', 'circulation', 'circumstance', \"circus-clown's\", 'claimed', 'clasping', 'clear', 'cleverer', 'close', 'clue', 'coat', 'collapsed', 'colour', 'come', 'comfortable', 'coming', 'companion', 'compared', 'complex', 'confident', 'congesting', 'conjugal', 'constraint', 'consummate', 'contended', 'continued', 'corner', 'corrected', 'could', \"couldn't\", 'count', 'countenance', 'couple', 'course', 'covered', 'craft', 'cried', 'crossed', 'crowned', 'crumbled', 'cry', 'cured', 'curiosity', 'curious', 'current', 'curtains', 'dabble', 'damask', 'dark', 'dashed', 'day', 'days', 'dead', 'deadening', 'dear', 'deep', \"deerhound's\", 'degree', 'delicate', 'demand', 'denied', 'deploring', 'deprecating', 'deprecatingly', 'desire', 'destroyed', 'destruction', 'desultory', 'detail', 'diagnosis', 'did', \"didn't\", 'died', 'dim', 'dimmest', 'dingy', 'dining-room', 'disarming', 'discovery', 'discrimination', 'discussion', 'disdain', 'disdained', 'disease', 'disguised', 'display', 'dissatisfied', 'distinguished', 'distract', 'divert', 'do', \"doesn't\", 'doing', 'domestic', \"don't\", 'done', 'donkey', 'down', 'dozen', 'dragged', 'drawing-room', 'drawing-rooms', 'drawn', 'dress-closets', 'drew', 'dropped', 'each', 'earth', 'ease', 'easel', 'easy', 'echoed', 'economy', 'effect', 'effects', 'efforts', 'egregious', 'eighteenth-century', 'elbow', 'elegant', 'else', 'embarrassed', 'enabled', 'end', 'endless', 'enjoy', 'enlightenment', 'enough', 'ensuing', 'equally', 'equanimity', 'escape', 'established', 'etching', 'even', 'event', 'ever', 'everlasting', 'every', 'exasperated', 'except', 'excuse', 'excusing', 'existed', 'expected', 'exquisite', 'exquisitely', 'extenuation', 'exterminating', 'extracting', 'eye', 'eyebrows', 'eyes', 'face', 'faces', 'fact', 'faded', 'failed', 'failure', 'fair', 'faith', 'false', 'familiar', 'fancy', 'fashionable', 'fate', 'feather', 'feet', 'fell', 'fellow', 'felt', 'few', 'fewer', 'finality', 'find', 'fingers', 'first', 'fit', 'fitting', 'five', 'flash', 'flashed', 'florid', 'flowers', 'fluently', 'flung', 'follow', 'followed', 'fond', 'footstep', 'for', 'forced', 'forcing', 'forehead', 'foreign', 'foreseen', 'forgive', 'forgotten', 'form', 'formed', 'forming', 'forward', 'fostered', 'found', 'foundations', 'fragment', 'fragments', 'frame', 'frames', 'frequently', \"friend's\", 'from', 'full', 'fullest', 'furiously', 'furrowed', 'garlanded', 'garlands', 'gave', 'genial', 'genius', 'gesture', 'get', 'getting', 'give', 'given', 'glad', 'glanced', 'glimpse', 'gloried', 'glory', 'go', 'going', 'gone', 'good', 'good-breeding', 'good-humoured', 'got', 'grace', 'gradually', 'gray', 'grayish', 'great', 'greatest', 'greatness', 'grew', 'groping', 'growing', 'had', \"hadn't\", 'hair', 'half', 'half-light', 'half-mechanically', 'hall', 'hand', 'hands', 'handsome', 'hanging', 'happen', 'happened', 'hard', 'hardly', 'have', \"haven't\", 'having', 'he', \"he'd\", \"he's\", 'head', 'hear', 'heard', 'heart', 'height', 'her', 'here', 'hermit', 'herself', 'hesitations', 'hide', 'high', 'him', 'himself', 'hint', 'his', 'history', 'holding', 'home', 'honour', 'hooded', 'hostess', 'hot-house', 'hour', 'hours', 'house', 'how', 'hung', 'husband', \"husband's\", 'idea', 'idle', 'idling', 'if', 'immediately', 'in', 'incense', 'indifferent', 'inevitable', 'inevitably', 'inflexible', 'insensible', 'insignificant', 'instinctively', 'instructive', 'interesting', 'into', 'ironic', 'irony', 'irrelevance', 'irrevocable', 'is', 'it', \"it's\", 'its', 'itself', 'jealousy', 'just', 'keep', 'kept', 'kind', 'knees', 'knew', 'know', 'known_', 'laid', 'lair', 'landing', 'language', 'last', 'late', 'later', 'latter', \"latter's\", 'laugh', 'laughed', 'lay', 'leading', 'lean', 'learned', 'least', 'leathery', 'leave', 'led', 'left', 'leisure', 'lends', 'lent', 'let', 'lies', 'life', 'life-likeness', 'lift', 'lifted', 'light', 'lightly', 'like', 'liked', 'line', 'lines', 'lingered', 'lips', 'lit', 'little', 'live', 'loathing', 'long', 'longed', 'longer', 'look', 'looked', 'looking', 'lose', 'loss', 'lounging', 'lovely', 'lucky', 'lump', 'luncheon-table', 'luxury', 'lying', 'made', 'make', 'man', 'manage', 'managed', 'mantel-piece', 'marble', 'married', 'may', 'me', 'meant', 'mediocrity', 'medium', 'mentioned', 'mere', 'merely', 'met', 'might', 'mighty', \"millionaire's\", 'mine', 'minute', 'minutes', 'mirrors', 'modest', 'modesty', 'moment', 'money', 'monumental', 'mood', 'morbidly', 'more', 'most', 'mourn', 'mourned', 'moustache', 'moved', 'much', 'muddling', 'multiplied', 'murmur', 'muscles', 'must', 'my', 'myself', 'mysterious', 'naive', 'near', 'nearly', 'negatived', 'nervous', 'nervousness', 'neutral', 'never', 'next', 'no', 'none', 'not', 'note', 'nothing', 'now', 'nymphs', 'oak', 'obituary', 'object', 'objects', 'occurred', 'oddly', 'of', 'off', 'often', 'oh', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'open', 'or', 'other', 'our', 'ourselves', 'out', 'outline', 'oval', 'over', 'own', 'packed', 'paid', 'paint', 'painted', 'painter', 'painting', 'pale', 'paled', 'palm-trees', 'panel', 'panelling', 'pardonable', 'pardoned', 'part', 'passages', 'passing', 'past', 'pastels', 'pathos', 'patient', 'people', 'perceptible', 'perfect', 'persistence', 'persuasively', 'phrase', 'picture', 'pictures', 'pines', 'pink', 'place', 'placed', 'plain', 'platitudes', 'pleased', 'pockets', 'point', 'poised', 'poor', 'portrait', 'posing', 'possessed', 'poverty', 'predicted', 'preliminary', 'presenting', 'prestidigitation', 'pretty', 'previous', 'price', 'pride', 'princely', 'prism', 'problem', 'proclaiming', 'prodigious', 'profusion', 'protest', 'prove', 'public', 'purblind', 'purely', 'pushed', 'put', 'qualities', 'quality', 'queerly', 'question', 'quickly', 'quietly', 'quite', 'quote', 'rain', 'raised', 'random', 'rather', 'real', 'really', 'reared', 'reason', 'reassurance', 'recovering', 'recreated', 'reflected', 'reflection', 'regrets', 'relatively', 'remained', 'remember', 'reminded', 'repeating', 'represented', 'reproduction', 'resented', 'resolve', 'resources', 'rest', 'rich', 'ridiculous', 'robbed', 'romantic', 'room', 'rose', 'rule', 'run', 'said', 'same', 'satisfaction', 'savour', 'saw', 'say', 'saying', 'says', 'scorn', 'scornful', 'secret', 'see', 'seemed', 'seen', 'self-confident', 'send', 'sensation', 'sensitive', 'sent', 'serious', 'set', 'sex', 'shade', 'shaking', 'shall', 'she', \"she's\", 'shirked', 'short', 'should', 'shoulder', 'shoulders', 'show', 'showed', 'showy', 'shrug', 'shrugged', 'sight', 'sign', 'silent', 'silver', 'similar', 'simpleton', 'simplifications', 'simply', 'since', 'single', 'sitter', 'sitters', 'sketch', 'skill', 'slight', 'slightly', 'slowly', 'small', 'smile', 'smiling', 'sneer', 'so', 'solace', 'some', 'somebody', 'something', 'spacious', 'spaniel', 'speaking-tubes', 'speculations', 'spite', 'splash', 'square', 'stairs', 'stammer', 'stand', 'standing', 'started', 'stay', 'still', 'stocked', 'stood', 'stopped', 'stopping', 'straddling', 'straight', 'strain', 'straining', 'strange', 'straw', 'stream', 'stroke', 'strokes', 'strolled', 'strongest', 'strongly', 'struck', 'studio', 'stuff', 'subject', 'substantial', 'suburban', 'such', 'suddenly', 'suffered', 'sugar', 'suggested', 'sunburn', 'sunburnt', 'sunlit', 'superb', 'sure', 'surest', 'surface', 'surprise', 'surprised', 'surrounded', 'suspected', 'sweetly', 'sweetness', 'swelling', 'swept', 'swum', 'table', 'take', 'taken', 'talking', 'tea', 'tears', 'technicalities', 'technique', 'tell', 'tells', 'tempting', 'terra-cotta', 'terrace', 'terraces', 'terribly', 'than', 'that', 'the', 'their', 'them', 'then', 'there', \"there's\", 'therefore', 'they', \"they're\", 'thin', 'thing', 'things', 'think', 'this', 'thither', 'those', 'though', 'thought', 'three', 'threshold', 'threw', 'through', 'throwing', 'tie', 'till', 'time', 'timorously', 'tinge', 'tips', 'tired', 'to', 'told', 'tone', 'tones', 'too', 'took', 'tottering', 'touched', 'toward', 'trace', 'trade', 'transmute', 'traps', 'travelled', 'tribute', 'tributes', 'tricks', 'tried', 'trouser-presses', 'true', 'truth', 'turned', 'twenty', 'twenty-four', 'twice', 'twirling', 'unaccountable', 'uncertain', 'under', 'underlay', 'underneath', 'understand', 'unexpected', 'untouched', 'unusual', 'up', 'up-stream', 'upon', 'upset', 'upstairs', 'us', 'used', 'usual', 'value', 'varnishing', 'vases', 'veins', 'velveteen', 'very', 'villa', 'vindicated', 'virtuosity', 'vista', 'vocation', 'voice', 'wall', 'wander', 'want', 'wanted', 'wants', 'was', \"wasn't\", 'watched', 'watching', 'water-colour', 'waves', 'way', 'weekly', 'weeks', 'welcome', 'went', 'were', 'what', 'when', 'whenever', 'where', 'which', 'while', 'white', 'white-panelled', 'who', 'whole', 'whom', 'why', 'wide', 'widow', 'wife', \"wife's\", 'wild', 'wincing', 'window-curtains', 'wish', 'with', 'without', 'wits', 'woman', 'women', \"won't\", 'wonder', 'wondered', 'word', 'work', 'working', 'worth', 'would', \"wouldn't\", 'year', 'years', 'yellow', 'yet', 'you', \"you'd\", \"you're\", 'younger', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "957ac860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'A': 10, 'Ah': 11, 'Among': 12, 'And': 13, 'Are': 14, 'Arrt': 15, 'As': 16, 'At': 17, 'Be': 18, 'Begin': 19, 'Burlington': 20, 'But': 21, 'By': 22, 'Carlo': 23, 'Chicago': 24, 'Claude': 25, 'Come': 26, 'Croft': 27, 'Destroyed': 28, 'Devonshire': 29, \"Don't\": 30, 'Dubarry_': 31, 'Emperors': 32, 'Florence': 33, 'For': 34, 'Gallery': 35, 'Gideon': 36, 'Gisburn': 37, \"Gisburn's\": 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, \"Grindle's\": 43, 'Grindles': 44, 'HAD': 45, 'Had': 46, 'Hang': 47, 'Has': 48, 'He': 49, 'Her': 50, 'Hermia': 51, \"Hermia's\": 52, 'His': 53, 'How': 54, 'I': 55, \"I'd\": 56, \"I'll\": 57, \"I've\": 58, 'If': 59, 'In': 60, 'It': 61, \"It's\": 62, 'Jack': 63, \"Jack's\": 64, 'Jove': 65, 'Just': 66, 'Lord': 67, 'Made': 68, 'Miss': 69, \"Money's\": 70, 'Monte': 71, 'Moon-dancers': 72, 'Mr': 73, 'Mrs': 74, 'My': 75, 'Never': 76, 'No': 77, 'Now': 78, 'Nutley': 79, 'Of': 80, 'Oh': 81, 'On': 82, 'Once': 83, 'Only': 84, 'Or': 85, 'Perhaps': 86, 'Poor': 87, 'Professional': 88, 'Renaissance': 89, 'Rickham': 90, 'Riviera': 91, 'Rome': 92, 'Russian': 93, 'Sevres': 94, 'She': 95, \"She's\": 96, 'Stroud': 97, \"Stroud's\": 98, 'Strouds': 99, 'Suddenly': 100, 'That': 101, \"That's\": 102, 'The': 103, 'Then': 104, 'There': 105, 'They': 106, 'This': 107, 'Those': 108, 'Though': 109, 'Thwing': 110, \"Thwing's\": 111, 'Thwings': 112, 'To': 113, 'Usually': 114, 'Venetian': 115, 'Victor': 116, 'Was': 117, 'We': 118, 'Well': 119, 'What': 120, 'When': 121, 'Why': 122, 'Yes': 123, 'You': 124, '_I': 125, '_am_': 126, '_famille-verte_': 127, '_felt_': 128, '_has_': 129, '_have_': 130, '_jardiniere_': 131, '_mine_': 132, '_not_': 133, '_rose': 134, '_rs_': 135, '_that_': 136, '_the_': 137, '_was_': 138, '_were_': 139, 'a': 140, 'abdication': 141, 'able': 142, 'about': 143, 'above': 144, 'abruptly': 145, 'absolute': 146, 'absorbed': 147, 'absurdity': 148, 'academic': 149, 'accuse': 150, 'accustomed': 151, 'across': 152, 'activity': 153, 'add': 154, 'added': 155, 'admirers': 156, 'adopted': 157, 'adulation': 158, 'advance': 159, 'aesthetic': 160, 'affect': 161, 'afraid': 162, 'after': 163, 'afterward': 164, 'again': 165, 'ago': 166, 'ah': 167, 'air': 168, 'alive': 169, 'all': 170, 'almost': 171, 'alone': 172, 'along': 173, 'always': 174, 'amazement': 175, 'amid': 176, 'among': 177, 'amplest': 178, 'amusing': 179, 'an': 180, 'and': 181, 'another': 182, 'answer': 183, 'answered': 184, 'any': 185, 'anything': 186, 'anywhere': 187, 'apparent': 188, 'apparently': 189, 'appearance': 190, 'appeared': 191, 'appointed': 192, 'are': 193, 'arm': 194, 'arm-chair': 195, 'arm-chairs': 196, 'arms': 197, 'art': 198, 'articles': 199, 'artist': 200, 'as': 201, 'aside': 202, 'asked': 203, 'at': 204, 'atmosphere': 205, 'atom': 206, 'attack': 207, 'attention': 208, 'attitude': 209, 'audacities': 210, 'away': 211, 'awful': 212, 'axioms': 213, 'azaleas': 214, 'back': 215, 'background': 216, 'balance': 217, 'balancing': 218, 'balustraded': 219, 'basking': 220, 'bath-rooms': 221, 'be': 222, 'beaming': 223, 'bean-stalk': 224, 'bear': 225, 'beard': 226, 'beauty': 227, 'became': 228, 'because': 229, 'becoming': 230, 'bed': 231, 'been': 232, 'before': 233, 'began': 234, 'begun': 235, 'behind': 236, 'being': 237, 'believed': 238, 'beneath': 239, 'bespoke': 240, 'better': 241, 'between': 242, 'big': 243, 'bits': 244, 'bitterness': 245, 'blocked': 246, 'born': 247, 'borne': 248, 'boudoir': 249, 'bravura': 250, 'break': 251, 'breaking': 252, 'breathing': 253, 'bric-a-brac': 254, 'briefly': 255, 'brings': 256, 'bronzes': 257, 'brought': 258, 'brown': 259, 'brush': 260, 'bull': 261, 'business': 262, 'but': 263, 'buying': 264, 'by': 265, 'called': 266, 'came': 267, 'can': 268, 'canvas': 269, 'canvases': 270, 'cards': 271, 'care': 272, 'career': 273, 'caught': 274, 'central': 275, 'chair': 276, 'chap': 277, 'characteristic': 278, 'charming': 279, 'cheap': 280, 'check': 281, 'cheeks': 282, 'chest': 283, 'chimney-piece': 284, 'chucked': 285, 'cigar': 286, 'cigarette': 287, 'cigars': 288, 'circulation': 289, 'circumstance': 290, \"circus-clown's\": 291, 'claimed': 292, 'clasping': 293, 'clear': 294, 'cleverer': 295, 'close': 296, 'clue': 297, 'coat': 298, 'collapsed': 299, 'colour': 300, 'come': 301, 'comfortable': 302, 'coming': 303, 'companion': 304, 'compared': 305, 'complex': 306, 'confident': 307, 'congesting': 308, 'conjugal': 309, 'constraint': 310, 'consummate': 311, 'contended': 312, 'continued': 313, 'corner': 314, 'corrected': 315, 'could': 316, \"couldn't\": 317, 'count': 318, 'countenance': 319, 'couple': 320, 'course': 321, 'covered': 322, 'craft': 323, 'cried': 324, 'crossed': 325, 'crowned': 326, 'crumbled': 327, 'cry': 328, 'cured': 329, 'curiosity': 330, 'curious': 331, 'current': 332, 'curtains': 333, 'dabble': 334, 'damask': 335, 'dark': 336, 'dashed': 337, 'day': 338, 'days': 339, 'dead': 340, 'deadening': 341, 'dear': 342, 'deep': 343, \"deerhound's\": 344, 'degree': 345, 'delicate': 346, 'demand': 347, 'denied': 348, 'deploring': 349, 'deprecating': 350, 'deprecatingly': 351, 'desire': 352, 'destroyed': 353, 'destruction': 354, 'desultory': 355, 'detail': 356, 'diagnosis': 357, 'did': 358, \"didn't\": 359, 'died': 360, 'dim': 361, 'dimmest': 362, 'dingy': 363, 'dining-room': 364, 'disarming': 365, 'discovery': 366, 'discrimination': 367, 'discussion': 368, 'disdain': 369, 'disdained': 370, 'disease': 371, 'disguised': 372, 'display': 373, 'dissatisfied': 374, 'distinguished': 375, 'distract': 376, 'divert': 377, 'do': 378, \"doesn't\": 379, 'doing': 380, 'domestic': 381, \"don't\": 382, 'done': 383, 'donkey': 384, 'down': 385, 'dozen': 386, 'dragged': 387, 'drawing-room': 388, 'drawing-rooms': 389, 'drawn': 390, 'dress-closets': 391, 'drew': 392, 'dropped': 393, 'each': 394, 'earth': 395, 'ease': 396, 'easel': 397, 'easy': 398, 'echoed': 399, 'economy': 400, 'effect': 401, 'effects': 402, 'efforts': 403, 'egregious': 404, 'eighteenth-century': 405, 'elbow': 406, 'elegant': 407, 'else': 408, 'embarrassed': 409, 'enabled': 410, 'end': 411, 'endless': 412, 'enjoy': 413, 'enlightenment': 414, 'enough': 415, 'ensuing': 416, 'equally': 417, 'equanimity': 418, 'escape': 419, 'established': 420, 'etching': 421, 'even': 422, 'event': 423, 'ever': 424, 'everlasting': 425, 'every': 426, 'exasperated': 427, 'except': 428, 'excuse': 429, 'excusing': 430, 'existed': 431, 'expected': 432, 'exquisite': 433, 'exquisitely': 434, 'extenuation': 435, 'exterminating': 436, 'extracting': 437, 'eye': 438, 'eyebrows': 439, 'eyes': 440, 'face': 441, 'faces': 442, 'fact': 443, 'faded': 444, 'failed': 445, 'failure': 446, 'fair': 447, 'faith': 448, 'false': 449, 'familiar': 450, 'fancy': 451, 'fashionable': 452, 'fate': 453, 'feather': 454, 'feet': 455, 'fell': 456, 'fellow': 457, 'felt': 458, 'few': 459, 'fewer': 460, 'finality': 461, 'find': 462, 'fingers': 463, 'first': 464, 'fit': 465, 'fitting': 466, 'five': 467, 'flash': 468, 'flashed': 469, 'florid': 470, 'flowers': 471, 'fluently': 472, 'flung': 473, 'follow': 474, 'followed': 475, 'fond': 476, 'footstep': 477, 'for': 478, 'forced': 479, 'forcing': 480, 'forehead': 481, 'foreign': 482, 'foreseen': 483, 'forgive': 484, 'forgotten': 485, 'form': 486, 'formed': 487, 'forming': 488, 'forward': 489, 'fostered': 490, 'found': 491, 'foundations': 492, 'fragment': 493, 'fragments': 494, 'frame': 495, 'frames': 496, 'frequently': 497, \"friend's\": 498, 'from': 499, 'full': 500, 'fullest': 501, 'furiously': 502, 'furrowed': 503, 'garlanded': 504, 'garlands': 505, 'gave': 506, 'genial': 507, 'genius': 508, 'gesture': 509, 'get': 510, 'getting': 511, 'give': 512, 'given': 513, 'glad': 514, 'glanced': 515, 'glimpse': 516, 'gloried': 517, 'glory': 518, 'go': 519, 'going': 520, 'gone': 521, 'good': 522, 'good-breeding': 523, 'good-humoured': 524, 'got': 525, 'grace': 526, 'gradually': 527, 'gray': 528, 'grayish': 529, 'great': 530, 'greatest': 531, 'greatness': 532, 'grew': 533, 'groping': 534, 'growing': 535, 'had': 536, \"hadn't\": 537, 'hair': 538, 'half': 539, 'half-light': 540, 'half-mechanically': 541, 'hall': 542, 'hand': 543, 'hands': 544, 'handsome': 545, 'hanging': 546, 'happen': 547, 'happened': 548, 'hard': 549, 'hardly': 550, 'have': 551, \"haven't\": 552, 'having': 553, 'he': 554, \"he'd\": 555, \"he's\": 556, 'head': 557, 'hear': 558, 'heard': 559, 'heart': 560, 'height': 561, 'her': 562, 'here': 563, 'hermit': 564, 'herself': 565, 'hesitations': 566, 'hide': 567, 'high': 568, 'him': 569, 'himself': 570, 'hint': 571, 'his': 572, 'history': 573, 'holding': 574, 'home': 575, 'honour': 576, 'hooded': 577, 'hostess': 578, 'hot-house': 579, 'hour': 580, 'hours': 581, 'house': 582, 'how': 583, 'hung': 584, 'husband': 585, \"husband's\": 586, 'idea': 587, 'idle': 588, 'idling': 589, 'if': 590, 'immediately': 591, 'in': 592, 'incense': 593, 'indifferent': 594, 'inevitable': 595, 'inevitably': 596, 'inflexible': 597, 'insensible': 598, 'insignificant': 599, 'instinctively': 600, 'instructive': 601, 'interesting': 602, 'into': 603, 'ironic': 604, 'irony': 605, 'irrelevance': 606, 'irrevocable': 607, 'is': 608, 'it': 609, \"it's\": 610, 'its': 611, 'itself': 612, 'jealousy': 613, 'just': 614, 'keep': 615, 'kept': 616, 'kind': 617, 'knees': 618, 'knew': 619, 'know': 620, 'known_': 621, 'laid': 622, 'lair': 623, 'landing': 624, 'language': 625, 'last': 626, 'late': 627, 'later': 628, 'latter': 629, \"latter's\": 630, 'laugh': 631, 'laughed': 632, 'lay': 633, 'leading': 634, 'lean': 635, 'learned': 636, 'least': 637, 'leathery': 638, 'leave': 639, 'led': 640, 'left': 641, 'leisure': 642, 'lends': 643, 'lent': 644, 'let': 645, 'lies': 646, 'life': 647, 'life-likeness': 648, 'lift': 649, 'lifted': 650, 'light': 651, 'lightly': 652, 'like': 653, 'liked': 654, 'line': 655, 'lines': 656, 'lingered': 657, 'lips': 658, 'lit': 659, 'little': 660, 'live': 661, 'loathing': 662, 'long': 663, 'longed': 664, 'longer': 665, 'look': 666, 'looked': 667, 'looking': 668, 'lose': 669, 'loss': 670, 'lounging': 671, 'lovely': 672, 'lucky': 673, 'lump': 674, 'luncheon-table': 675, 'luxury': 676, 'lying': 677, 'made': 678, 'make': 679, 'man': 680, 'manage': 681, 'managed': 682, 'mantel-piece': 683, 'marble': 684, 'married': 685, 'may': 686, 'me': 687, 'meant': 688, 'mediocrity': 689, 'medium': 690, 'mentioned': 691, 'mere': 692, 'merely': 693, 'met': 694, 'might': 695, 'mighty': 696, \"millionaire's\": 697, 'mine': 698, 'minute': 699, 'minutes': 700, 'mirrors': 701, 'modest': 702, 'modesty': 703, 'moment': 704, 'money': 705, 'monumental': 706, 'mood': 707, 'morbidly': 708, 'more': 709, 'most': 710, 'mourn': 711, 'mourned': 712, 'moustache': 713, 'moved': 714, 'much': 715, 'muddling': 716, 'multiplied': 717, 'murmur': 718, 'muscles': 719, 'must': 720, 'my': 721, 'myself': 722, 'mysterious': 723, 'naive': 724, 'near': 725, 'nearly': 726, 'negatived': 727, 'nervous': 728, 'nervousness': 729, 'neutral': 730, 'never': 731, 'next': 732, 'no': 733, 'none': 734, 'not': 735, 'note': 736, 'nothing': 737, 'now': 738, 'nymphs': 739, 'oak': 740, 'obituary': 741, 'object': 742, 'objects': 743, 'occurred': 744, 'oddly': 745, 'of': 746, 'off': 747, 'often': 748, 'oh': 749, 'old': 750, 'on': 751, 'once': 752, 'one': 753, 'ones': 754, 'only': 755, 'onto': 756, 'open': 757, 'or': 758, 'other': 759, 'our': 760, 'ourselves': 761, 'out': 762, 'outline': 763, 'oval': 764, 'over': 765, 'own': 766, 'packed': 767, 'paid': 768, 'paint': 769, 'painted': 770, 'painter': 771, 'painting': 772, 'pale': 773, 'paled': 774, 'palm-trees': 775, 'panel': 776, 'panelling': 777, 'pardonable': 778, 'pardoned': 779, 'part': 780, 'passages': 781, 'passing': 782, 'past': 783, 'pastels': 784, 'pathos': 785, 'patient': 786, 'people': 787, 'perceptible': 788, 'perfect': 789, 'persistence': 790, 'persuasively': 791, 'phrase': 792, 'picture': 793, 'pictures': 794, 'pines': 795, 'pink': 796, 'place': 797, 'placed': 798, 'plain': 799, 'platitudes': 800, 'pleased': 801, 'pockets': 802, 'point': 803, 'poised': 804, 'poor': 805, 'portrait': 806, 'posing': 807, 'possessed': 808, 'poverty': 809, 'predicted': 810, 'preliminary': 811, 'presenting': 812, 'prestidigitation': 813, 'pretty': 814, 'previous': 815, 'price': 816, 'pride': 817, 'princely': 818, 'prism': 819, 'problem': 820, 'proclaiming': 821, 'prodigious': 822, 'profusion': 823, 'protest': 824, 'prove': 825, 'public': 826, 'purblind': 827, 'purely': 828, 'pushed': 829, 'put': 830, 'qualities': 831, 'quality': 832, 'queerly': 833, 'question': 834, 'quickly': 835, 'quietly': 836, 'quite': 837, 'quote': 838, 'rain': 839, 'raised': 840, 'random': 841, 'rather': 842, 'real': 843, 'really': 844, 'reared': 845, 'reason': 846, 'reassurance': 847, 'recovering': 848, 'recreated': 849, 'reflected': 850, 'reflection': 851, 'regrets': 852, 'relatively': 853, 'remained': 854, 'remember': 855, 'reminded': 856, 'repeating': 857, 'represented': 858, 'reproduction': 859, 'resented': 860, 'resolve': 861, 'resources': 862, 'rest': 863, 'rich': 864, 'ridiculous': 865, 'robbed': 866, 'romantic': 867, 'room': 868, 'rose': 869, 'rule': 870, 'run': 871, 'said': 872, 'same': 873, 'satisfaction': 874, 'savour': 875, 'saw': 876, 'say': 877, 'saying': 878, 'says': 879, 'scorn': 880, 'scornful': 881, 'secret': 882, 'see': 883, 'seemed': 884, 'seen': 885, 'self-confident': 886, 'send': 887, 'sensation': 888, 'sensitive': 889, 'sent': 890, 'serious': 891, 'set': 892, 'sex': 893, 'shade': 894, 'shaking': 895, 'shall': 896, 'she': 897, \"she's\": 898, 'shirked': 899, 'short': 900, 'should': 901, 'shoulder': 902, 'shoulders': 903, 'show': 904, 'showed': 905, 'showy': 906, 'shrug': 907, 'shrugged': 908, 'sight': 909, 'sign': 910, 'silent': 911, 'silver': 912, 'similar': 913, 'simpleton': 914, 'simplifications': 915, 'simply': 916, 'since': 917, 'single': 918, 'sitter': 919, 'sitters': 920, 'sketch': 921, 'skill': 922, 'slight': 923, 'slightly': 924, 'slowly': 925, 'small': 926, 'smile': 927, 'smiling': 928, 'sneer': 929, 'so': 930, 'solace': 931, 'some': 932, 'somebody': 933, 'something': 934, 'spacious': 935, 'spaniel': 936, 'speaking-tubes': 937, 'speculations': 938, 'spite': 939, 'splash': 940, 'square': 941, 'stairs': 942, 'stammer': 943, 'stand': 944, 'standing': 945, 'started': 946, 'stay': 947, 'still': 948, 'stocked': 949, 'stood': 950, 'stopped': 951, 'stopping': 952, 'straddling': 953, 'straight': 954, 'strain': 955, 'straining': 956, 'strange': 957, 'straw': 958, 'stream': 959, 'stroke': 960, 'strokes': 961, 'strolled': 962, 'strongest': 963, 'strongly': 964, 'struck': 965, 'studio': 966, 'stuff': 967, 'subject': 968, 'substantial': 969, 'suburban': 970, 'such': 971, 'suddenly': 972, 'suffered': 973, 'sugar': 974, 'suggested': 975, 'sunburn': 976, 'sunburnt': 977, 'sunlit': 978, 'superb': 979, 'sure': 980, 'surest': 981, 'surface': 982, 'surprise': 983, 'surprised': 984, 'surrounded': 985, 'suspected': 986, 'sweetly': 987, 'sweetness': 988, 'swelling': 989, 'swept': 990, 'swum': 991, 'table': 992, 'take': 993, 'taken': 994, 'talking': 995, 'tea': 996, 'tears': 997, 'technicalities': 998, 'technique': 999, 'tell': 1000, 'tells': 1001, 'tempting': 1002, 'terra-cotta': 1003, 'terrace': 1004, 'terraces': 1005, 'terribly': 1006, 'than': 1007, 'that': 1008, 'the': 1009, 'their': 1010, 'them': 1011, 'then': 1012, 'there': 1013, \"there's\": 1014, 'therefore': 1015, 'they': 1016, \"they're\": 1017, 'thin': 1018, 'thing': 1019, 'things': 1020, 'think': 1021, 'this': 1022, 'thither': 1023, 'those': 1024, 'though': 1025, 'thought': 1026, 'three': 1027, 'threshold': 1028, 'threw': 1029, 'through': 1030, 'throwing': 1031, 'tie': 1032, 'till': 1033, 'time': 1034, 'timorously': 1035, 'tinge': 1036, 'tips': 1037, 'tired': 1038, 'to': 1039, 'told': 1040, 'tone': 1041, 'tones': 1042, 'too': 1043, 'took': 1044, 'tottering': 1045, 'touched': 1046, 'toward': 1047, 'trace': 1048, 'trade': 1049, 'transmute': 1050, 'traps': 1051, 'travelled': 1052, 'tribute': 1053, 'tributes': 1054, 'tricks': 1055, 'tried': 1056, 'trouser-presses': 1057, 'true': 1058, 'truth': 1059, 'turned': 1060, 'twenty': 1061, 'twenty-four': 1062, 'twice': 1063, 'twirling': 1064, 'unaccountable': 1065, 'uncertain': 1066, 'under': 1067, 'underlay': 1068, 'underneath': 1069, 'understand': 1070, 'unexpected': 1071, 'untouched': 1072, 'unusual': 1073, 'up': 1074, 'up-stream': 1075, 'upon': 1076, 'upset': 1077, 'upstairs': 1078, 'us': 1079, 'used': 1080, 'usual': 1081, 'value': 1082, 'varnishing': 1083, 'vases': 1084, 'veins': 1085, 'velveteen': 1086, 'very': 1087, 'villa': 1088, 'vindicated': 1089, 'virtuosity': 1090, 'vista': 1091, 'vocation': 1092, 'voice': 1093, 'wall': 1094, 'wander': 1095, 'want': 1096, 'wanted': 1097, 'wants': 1098, 'was': 1099, \"wasn't\": 1100, 'watched': 1101, 'watching': 1102, 'water-colour': 1103, 'waves': 1104, 'way': 1105, 'weekly': 1106, 'weeks': 1107, 'welcome': 1108, 'went': 1109, 'were': 1110, 'what': 1111, 'when': 1112, 'whenever': 1113, 'where': 1114, 'which': 1115, 'while': 1116, 'white': 1117, 'white-panelled': 1118, 'who': 1119, 'whole': 1120, 'whom': 1121, 'why': 1122, 'wide': 1123, 'widow': 1124, 'wife': 1125, \"wife's\": 1126, 'wild': 1127, 'wincing': 1128, 'window-curtains': 1129, 'wish': 1130, 'with': 1131, 'without': 1132, 'wits': 1133, 'woman': 1134, 'women': 1135, \"won't\": 1136, 'wonder': 1137, 'wondered': 1138, 'word': 1139, 'work': 1140, 'working': 1141, 'worth': 1142, 'would': 1143, \"wouldn't\": 1144, 'year': 1145, 'years': 1146, 'yellow': 1147, 'yet': 1148, 'you': 1149, \"you'd\": 1150, \"you're\": 1151, 'younger': 1152, 'your': 1153, 'yourself': 1154}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "i = 0\n",
    "for word in all_words:\n",
    "    vocab[word] = i\n",
    "    i += 1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1eab2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "enumerate-> \n",
    "هم میخوای بدونی عنصر چیه مثلا چه کلمه ایه و هم موقعیتش کجاس\n",
    "\"\"\"\n",
    "for key, item in enumerate(vocab.items()):\n",
    "    # print(item)\n",
    "    if key >= 52:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05460c3",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">Listing 2.3 Implementing a simple text tokenizer\n",
    "</h1>\n",
    "#1 Stores the vocabulary as a class attribute for access in the encode\n",
    "and decode methods</br>\n",
    "#2 Creates an inverse vocabulary that maps token IDs back to the\n",
    "original text tokens</br>\n",
    "#3 Processes input text into token IDs</br>\n",
    "#4 Converts token IDs back into text</br>\n",
    "#5 Removes spaces before the specified punctuation</br>\n",
    "<span style=\"color:rgb(255, 255, 255);\">-----------------------------------------</span></br>\n",
    "<h1 style=\"color:rgb(253, 173, 119);\">2.4 Adding special context tokens : </h1>\n",
    "</br>add an <|unk|> token to represent new and</br>\n",
    "unknown words that were not part of the training data and thus not part</br>\n",
    "of the existing vocabulary. Furthermore, we add an <|endoftext|> token</br>\n",
    "that we can use to separate two unrelated text sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30d1859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        preprocessed_org = re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|[.,:;!?\\\"()\\[\\]—–-]\", vocab)\n",
    "        sorted_word = sorted(list(set(preprocessed_org)))\n",
    "        sorted_word.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "        vocab = {token:integer for integer, token in enumerate(sorted_word)}\n",
    "        print(vocab)\n",
    "        for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "            print(item)\n",
    "\n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "# For new text\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.findall(r\"<\\|.*?\\|>|[\\w']+|[.,!?;]\", text)\n",
    "        ids = []\n",
    "        for token in preprocessed:\n",
    "            if token in self.str_to_int:\n",
    "                ids.append(self.str_to_int[token])\n",
    "            else:\n",
    "                ids.append(self.str_to_int[ \"<|unk|>\" ])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        \n",
    "        text = re.sub(r'\\s+([\\\\])', r'\\\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b314709",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(253, 173, 119); font-size:20px\">Wharton’s short story to try it out in practice:</span></br>\n",
    "The preceding code prints the following token IDs:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7244e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, '(': 2, ')': 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'A': 10, 'Ah': 11, 'Among': 12, 'And': 13, 'Are': 14, 'Arrt': 15, 'As': 16, 'At': 17, 'Be': 18, 'Begin': 19, 'Burlington': 20, 'But': 21, 'By': 22, 'Carlo': 23, 'Chicago': 24, 'Claude': 25, 'Come': 26, 'Croft': 27, 'Destroyed': 28, 'Devonshire': 29, \"Don't\": 30, 'Dubarry_': 31, 'Emperors': 32, 'Florence': 33, 'For': 34, 'Gallery': 35, 'Gideon': 36, 'Gisburn': 37, \"Gisburn's\": 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, \"Grindle's\": 43, 'Grindles': 44, 'HAD': 45, 'Had': 46, 'Hang': 47, 'Has': 48, 'He': 49, 'Her': 50, 'Hermia': 51, \"Hermia's\": 52, 'His': 53, 'How': 54, 'I': 55, \"I'd\": 56, \"I'll\": 57, \"I've\": 58, 'If': 59, 'In': 60, 'It': 61, \"It's\": 62, 'Jack': 63, \"Jack's\": 64, 'Jove': 65, 'Just': 66, 'Lord': 67, 'Made': 68, 'Miss': 69, \"Money's\": 70, 'Monte': 71, 'Moon-dancers': 72, 'Mr': 73, 'Mrs': 74, 'My': 75, 'Never': 76, 'No': 77, 'Now': 78, 'Nutley': 79, 'Of': 80, 'Oh': 81, 'On': 82, 'Once': 83, 'Only': 84, 'Or': 85, 'Perhaps': 86, 'Poor': 87, 'Professional': 88, 'Renaissance': 89, 'Rickham': 90, 'Riviera': 91, 'Rome': 92, 'Russian': 93, 'Sevres': 94, 'She': 95, \"She's\": 96, 'Stroud': 97, \"Stroud's\": 98, 'Strouds': 99, 'Suddenly': 100, 'That': 101, \"That's\": 102, 'The': 103, 'Then': 104, 'There': 105, 'They': 106, 'This': 107, 'Those': 108, 'Though': 109, 'Thwing': 110, \"Thwing's\": 111, 'Thwings': 112, 'To': 113, 'Usually': 114, 'Venetian': 115, 'Victor': 116, 'Was': 117, 'We': 118, 'Well': 119, 'What': 120, 'When': 121, 'Why': 122, 'Yes': 123, 'You': 124, '_I': 125, '_am_': 126, '_famille-verte_': 127, '_felt_': 128, '_has_': 129, '_have_': 130, '_jardiniere_': 131, '_mine_': 132, '_not_': 133, '_rose': 134, '_rs_': 135, '_that_': 136, '_the_': 137, '_was_': 138, '_were_': 139, 'a': 140, 'abdication': 141, 'able': 142, 'about': 143, 'above': 144, 'abruptly': 145, 'absolute': 146, 'absorbed': 147, 'absurdity': 148, 'academic': 149, 'accuse': 150, 'accustomed': 151, 'across': 152, 'activity': 153, 'add': 154, 'added': 155, 'admirers': 156, 'adopted': 157, 'adulation': 158, 'advance': 159, 'aesthetic': 160, 'affect': 161, 'afraid': 162, 'after': 163, 'afterward': 164, 'again': 165, 'ago': 166, 'ah': 167, 'air': 168, 'alive': 169, 'all': 170, 'almost': 171, 'alone': 172, 'along': 173, 'always': 174, 'amazement': 175, 'amid': 176, 'among': 177, 'amplest': 178, 'amusing': 179, 'an': 180, 'and': 181, 'another': 182, 'answer': 183, 'answered': 184, 'any': 185, 'anything': 186, 'anywhere': 187, 'apparent': 188, 'apparently': 189, 'appearance': 190, 'appeared': 191, 'appointed': 192, 'are': 193, 'arm': 194, 'arm-chair': 195, 'arm-chairs': 196, 'arms': 197, 'art': 198, 'articles': 199, 'artist': 200, 'as': 201, 'aside': 202, 'asked': 203, 'at': 204, 'atmosphere': 205, 'atom': 206, 'attack': 207, 'attention': 208, 'attitude': 209, 'audacities': 210, 'away': 211, 'awful': 212, 'axioms': 213, 'azaleas': 214, 'back': 215, 'background': 216, 'balance': 217, 'balancing': 218, 'balustraded': 219, 'basking': 220, 'bath-rooms': 221, 'be': 222, 'beaming': 223, 'bean-stalk': 224, 'bear': 225, 'beard': 226, 'beauty': 227, 'became': 228, 'because': 229, 'becoming': 230, 'bed': 231, 'been': 232, 'before': 233, 'began': 234, 'begun': 235, 'behind': 236, 'being': 237, 'believed': 238, 'beneath': 239, 'bespoke': 240, 'better': 241, 'between': 242, 'big': 243, 'bits': 244, 'bitterness': 245, 'blocked': 246, 'born': 247, 'borne': 248, 'boudoir': 249, 'bravura': 250, 'break': 251, 'breaking': 252, 'breathing': 253, 'bric-a-brac': 254, 'briefly': 255, 'brings': 256, 'bronzes': 257, 'brought': 258, 'brown': 259, 'brush': 260, 'bull': 261, 'business': 262, 'but': 263, 'buying': 264, 'by': 265, 'called': 266, 'came': 267, 'can': 268, 'canvas': 269, 'canvases': 270, 'cards': 271, 'care': 272, 'career': 273, 'caught': 274, 'central': 275, 'chair': 276, 'chap': 277, 'characteristic': 278, 'charming': 279, 'cheap': 280, 'check': 281, 'cheeks': 282, 'chest': 283, 'chimney-piece': 284, 'chucked': 285, 'cigar': 286, 'cigarette': 287, 'cigars': 288, 'circulation': 289, 'circumstance': 290, \"circus-clown's\": 291, 'claimed': 292, 'clasping': 293, 'clear': 294, 'cleverer': 295, 'close': 296, 'clue': 297, 'coat': 298, 'collapsed': 299, 'colour': 300, 'come': 301, 'comfortable': 302, 'coming': 303, 'companion': 304, 'compared': 305, 'complex': 306, 'confident': 307, 'congesting': 308, 'conjugal': 309, 'constraint': 310, 'consummate': 311, 'contended': 312, 'continued': 313, 'corner': 314, 'corrected': 315, 'could': 316, \"couldn't\": 317, 'count': 318, 'countenance': 319, 'couple': 320, 'course': 321, 'covered': 322, 'craft': 323, 'cried': 324, 'crossed': 325, 'crowned': 326, 'crumbled': 327, 'cry': 328, 'cured': 329, 'curiosity': 330, 'curious': 331, 'current': 332, 'curtains': 333, 'dabble': 334, 'damask': 335, 'dark': 336, 'dashed': 337, 'day': 338, 'days': 339, 'dead': 340, 'deadening': 341, 'dear': 342, 'deep': 343, \"deerhound's\": 344, 'degree': 345, 'delicate': 346, 'demand': 347, 'denied': 348, 'deploring': 349, 'deprecating': 350, 'deprecatingly': 351, 'desire': 352, 'destroyed': 353, 'destruction': 354, 'desultory': 355, 'detail': 356, 'diagnosis': 357, 'did': 358, \"didn't\": 359, 'died': 360, 'dim': 361, 'dimmest': 362, 'dingy': 363, 'dining-room': 364, 'disarming': 365, 'discovery': 366, 'discrimination': 367, 'discussion': 368, 'disdain': 369, 'disdained': 370, 'disease': 371, 'disguised': 372, 'display': 373, 'dissatisfied': 374, 'distinguished': 375, 'distract': 376, 'divert': 377, 'do': 378, \"doesn't\": 379, 'doing': 380, 'domestic': 381, \"don't\": 382, 'done': 383, 'donkey': 384, 'down': 385, 'dozen': 386, 'dragged': 387, 'drawing-room': 388, 'drawing-rooms': 389, 'drawn': 390, 'dress-closets': 391, 'drew': 392, 'dropped': 393, 'each': 394, 'earth': 395, 'ease': 396, 'easel': 397, 'easy': 398, 'echoed': 399, 'economy': 400, 'effect': 401, 'effects': 402, 'efforts': 403, 'egregious': 404, 'eighteenth-century': 405, 'elbow': 406, 'elegant': 407, 'else': 408, 'embarrassed': 409, 'enabled': 410, 'end': 411, 'endless': 412, 'enjoy': 413, 'enlightenment': 414, 'enough': 415, 'ensuing': 416, 'equally': 417, 'equanimity': 418, 'escape': 419, 'established': 420, 'etching': 421, 'even': 422, 'event': 423, 'ever': 424, 'everlasting': 425, 'every': 426, 'exasperated': 427, 'except': 428, 'excuse': 429, 'excusing': 430, 'existed': 431, 'expected': 432, 'exquisite': 433, 'exquisitely': 434, 'extenuation': 435, 'exterminating': 436, 'extracting': 437, 'eye': 438, 'eyebrows': 439, 'eyes': 440, 'face': 441, 'faces': 442, 'fact': 443, 'faded': 444, 'failed': 445, 'failure': 446, 'fair': 447, 'faith': 448, 'false': 449, 'familiar': 450, 'fancy': 451, 'fashionable': 452, 'fate': 453, 'feather': 454, 'feet': 455, 'fell': 456, 'fellow': 457, 'felt': 458, 'few': 459, 'fewer': 460, 'finality': 461, 'find': 462, 'fingers': 463, 'first': 464, 'fit': 465, 'fitting': 466, 'five': 467, 'flash': 468, 'flashed': 469, 'florid': 470, 'flowers': 471, 'fluently': 472, 'flung': 473, 'follow': 474, 'followed': 475, 'fond': 476, 'footstep': 477, 'for': 478, 'forced': 479, 'forcing': 480, 'forehead': 481, 'foreign': 482, 'foreseen': 483, 'forgive': 484, 'forgotten': 485, 'form': 486, 'formed': 487, 'forming': 488, 'forward': 489, 'fostered': 490, 'found': 491, 'foundations': 492, 'fragment': 493, 'fragments': 494, 'frame': 495, 'frames': 496, 'frequently': 497, \"friend's\": 498, 'from': 499, 'full': 500, 'fullest': 501, 'furiously': 502, 'furrowed': 503, 'garlanded': 504, 'garlands': 505, 'gave': 506, 'genial': 507, 'genius': 508, 'gesture': 509, 'get': 510, 'getting': 511, 'give': 512, 'given': 513, 'glad': 514, 'glanced': 515, 'glimpse': 516, 'gloried': 517, 'glory': 518, 'go': 519, 'going': 520, 'gone': 521, 'good': 522, 'good-breeding': 523, 'good-humoured': 524, 'got': 525, 'grace': 526, 'gradually': 527, 'gray': 528, 'grayish': 529, 'great': 530, 'greatest': 531, 'greatness': 532, 'grew': 533, 'groping': 534, 'growing': 535, 'had': 536, \"hadn't\": 537, 'hair': 538, 'half': 539, 'half-light': 540, 'half-mechanically': 541, 'hall': 542, 'hand': 543, 'hands': 544, 'handsome': 545, 'hanging': 546, 'happen': 547, 'happened': 548, 'hard': 549, 'hardly': 550, 'have': 551, \"haven't\": 552, 'having': 553, 'he': 554, \"he'd\": 555, \"he's\": 556, 'head': 557, 'hear': 558, 'heard': 559, 'heart': 560, 'height': 561, 'her': 562, 'here': 563, 'hermit': 564, 'herself': 565, 'hesitations': 566, 'hide': 567, 'high': 568, 'him': 569, 'himself': 570, 'hint': 571, 'his': 572, 'history': 573, 'holding': 574, 'home': 575, 'honour': 576, 'hooded': 577, 'hostess': 578, 'hot-house': 579, 'hour': 580, 'hours': 581, 'house': 582, 'how': 583, 'hung': 584, 'husband': 585, \"husband's\": 586, 'idea': 587, 'idle': 588, 'idling': 589, 'if': 590, 'immediately': 591, 'in': 592, 'incense': 593, 'indifferent': 594, 'inevitable': 595, 'inevitably': 596, 'inflexible': 597, 'insensible': 598, 'insignificant': 599, 'instinctively': 600, 'instructive': 601, 'interesting': 602, 'into': 603, 'ironic': 604, 'irony': 605, 'irrelevance': 606, 'irrevocable': 607, 'is': 608, 'it': 609, \"it's\": 610, 'its': 611, 'itself': 612, 'jealousy': 613, 'just': 614, 'keep': 615, 'kept': 616, 'kind': 617, 'knees': 618, 'knew': 619, 'know': 620, 'known_': 621, 'laid': 622, 'lair': 623, 'landing': 624, 'language': 625, 'last': 626, 'late': 627, 'later': 628, 'latter': 629, \"latter's\": 630, 'laugh': 631, 'laughed': 632, 'lay': 633, 'leading': 634, 'lean': 635, 'learned': 636, 'least': 637, 'leathery': 638, 'leave': 639, 'led': 640, 'left': 641, 'leisure': 642, 'lends': 643, 'lent': 644, 'let': 645, 'lies': 646, 'life': 647, 'life-likeness': 648, 'lift': 649, 'lifted': 650, 'light': 651, 'lightly': 652, 'like': 653, 'liked': 654, 'line': 655, 'lines': 656, 'lingered': 657, 'lips': 658, 'lit': 659, 'little': 660, 'live': 661, 'loathing': 662, 'long': 663, 'longed': 664, 'longer': 665, 'look': 666, 'looked': 667, 'looking': 668, 'lose': 669, 'loss': 670, 'lounging': 671, 'lovely': 672, 'lucky': 673, 'lump': 674, 'luncheon-table': 675, 'luxury': 676, 'lying': 677, 'made': 678, 'make': 679, 'man': 680, 'manage': 681, 'managed': 682, 'mantel-piece': 683, 'marble': 684, 'married': 685, 'may': 686, 'me': 687, 'meant': 688, 'mediocrity': 689, 'medium': 690, 'mentioned': 691, 'mere': 692, 'merely': 693, 'met': 694, 'might': 695, 'mighty': 696, \"millionaire's\": 697, 'mine': 698, 'minute': 699, 'minutes': 700, 'mirrors': 701, 'modest': 702, 'modesty': 703, 'moment': 704, 'money': 705, 'monumental': 706, 'mood': 707, 'morbidly': 708, 'more': 709, 'most': 710, 'mourn': 711, 'mourned': 712, 'moustache': 713, 'moved': 714, 'much': 715, 'muddling': 716, 'multiplied': 717, 'murmur': 718, 'muscles': 719, 'must': 720, 'my': 721, 'myself': 722, 'mysterious': 723, 'naive': 724, 'near': 725, 'nearly': 726, 'negatived': 727, 'nervous': 728, 'nervousness': 729, 'neutral': 730, 'never': 731, 'next': 732, 'no': 733, 'none': 734, 'not': 735, 'note': 736, 'nothing': 737, 'now': 738, 'nymphs': 739, 'oak': 740, 'obituary': 741, 'object': 742, 'objects': 743, 'occurred': 744, 'oddly': 745, 'of': 746, 'off': 747, 'often': 748, 'oh': 749, 'old': 750, 'on': 751, 'once': 752, 'one': 753, 'ones': 754, 'only': 755, 'onto': 756, 'open': 757, 'or': 758, 'other': 759, 'our': 760, 'ourselves': 761, 'out': 762, 'outline': 763, 'oval': 764, 'over': 765, 'own': 766, 'packed': 767, 'paid': 768, 'paint': 769, 'painted': 770, 'painter': 771, 'painting': 772, 'pale': 773, 'paled': 774, 'palm-trees': 775, 'panel': 776, 'panelling': 777, 'pardonable': 778, 'pardoned': 779, 'part': 780, 'passages': 781, 'passing': 782, 'past': 783, 'pastels': 784, 'pathos': 785, 'patient': 786, 'people': 787, 'perceptible': 788, 'perfect': 789, 'persistence': 790, 'persuasively': 791, 'phrase': 792, 'picture': 793, 'pictures': 794, 'pines': 795, 'pink': 796, 'place': 797, 'placed': 798, 'plain': 799, 'platitudes': 800, 'pleased': 801, 'pockets': 802, 'point': 803, 'poised': 804, 'poor': 805, 'portrait': 806, 'posing': 807, 'possessed': 808, 'poverty': 809, 'predicted': 810, 'preliminary': 811, 'presenting': 812, 'prestidigitation': 813, 'pretty': 814, 'previous': 815, 'price': 816, 'pride': 817, 'princely': 818, 'prism': 819, 'problem': 820, 'proclaiming': 821, 'prodigious': 822, 'profusion': 823, 'protest': 824, 'prove': 825, 'public': 826, 'purblind': 827, 'purely': 828, 'pushed': 829, 'put': 830, 'qualities': 831, 'quality': 832, 'queerly': 833, 'question': 834, 'quickly': 835, 'quietly': 836, 'quite': 837, 'quote': 838, 'rain': 839, 'raised': 840, 'random': 841, 'rather': 842, 'real': 843, 'really': 844, 'reared': 845, 'reason': 846, 'reassurance': 847, 'recovering': 848, 'recreated': 849, 'reflected': 850, 'reflection': 851, 'regrets': 852, 'relatively': 853, 'remained': 854, 'remember': 855, 'reminded': 856, 'repeating': 857, 'represented': 858, 'reproduction': 859, 'resented': 860, 'resolve': 861, 'resources': 862, 'rest': 863, 'rich': 864, 'ridiculous': 865, 'robbed': 866, 'romantic': 867, 'room': 868, 'rose': 869, 'rule': 870, 'run': 871, 'said': 872, 'same': 873, 'satisfaction': 874, 'savour': 875, 'saw': 876, 'say': 877, 'saying': 878, 'says': 879, 'scorn': 880, 'scornful': 881, 'secret': 882, 'see': 883, 'seemed': 884, 'seen': 885, 'self-confident': 886, 'send': 887, 'sensation': 888, 'sensitive': 889, 'sent': 890, 'serious': 891, 'set': 892, 'sex': 893, 'shade': 894, 'shaking': 895, 'shall': 896, 'she': 897, \"she's\": 898, 'shirked': 899, 'short': 900, 'should': 901, 'shoulder': 902, 'shoulders': 903, 'show': 904, 'showed': 905, 'showy': 906, 'shrug': 907, 'shrugged': 908, 'sight': 909, 'sign': 910, 'silent': 911, 'silver': 912, 'similar': 913, 'simpleton': 914, 'simplifications': 915, 'simply': 916, 'since': 917, 'single': 918, 'sitter': 919, 'sitters': 920, 'sketch': 921, 'skill': 922, 'slight': 923, 'slightly': 924, 'slowly': 925, 'small': 926, 'smile': 927, 'smiling': 928, 'sneer': 929, 'so': 930, 'solace': 931, 'some': 932, 'somebody': 933, 'something': 934, 'spacious': 935, 'spaniel': 936, 'speaking-tubes': 937, 'speculations': 938, 'spite': 939, 'splash': 940, 'square': 941, 'stairs': 942, 'stammer': 943, 'stand': 944, 'standing': 945, 'started': 946, 'stay': 947, 'still': 948, 'stocked': 949, 'stood': 950, 'stopped': 951, 'stopping': 952, 'straddling': 953, 'straight': 954, 'strain': 955, 'straining': 956, 'strange': 957, 'straw': 958, 'stream': 959, 'stroke': 960, 'strokes': 961, 'strolled': 962, 'strongest': 963, 'strongly': 964, 'struck': 965, 'studio': 966, 'stuff': 967, 'subject': 968, 'substantial': 969, 'suburban': 970, 'such': 971, 'suddenly': 972, 'suffered': 973, 'sugar': 974, 'suggested': 975, 'sunburn': 976, 'sunburnt': 977, 'sunlit': 978, 'superb': 979, 'sure': 980, 'surest': 981, 'surface': 982, 'surprise': 983, 'surprised': 984, 'surrounded': 985, 'suspected': 986, 'sweetly': 987, 'sweetness': 988, 'swelling': 989, 'swept': 990, 'swum': 991, 'table': 992, 'take': 993, 'taken': 994, 'talking': 995, 'tea': 996, 'tears': 997, 'technicalities': 998, 'technique': 999, 'tell': 1000, 'tells': 1001, 'tempting': 1002, 'terra-cotta': 1003, 'terrace': 1004, 'terraces': 1005, 'terribly': 1006, 'than': 1007, 'that': 1008, 'the': 1009, 'their': 1010, 'them': 1011, 'then': 1012, 'there': 1013, \"there's\": 1014, 'therefore': 1015, 'they': 1016, \"they're\": 1017, 'thin': 1018, 'thing': 1019, 'things': 1020, 'think': 1021, 'this': 1022, 'thither': 1023, 'those': 1024, 'though': 1025, 'thought': 1026, 'three': 1027, 'threshold': 1028, 'threw': 1029, 'through': 1030, 'throwing': 1031, 'tie': 1032, 'till': 1033, 'time': 1034, 'timorously': 1035, 'tinge': 1036, 'tips': 1037, 'tired': 1038, 'to': 1039, 'told': 1040, 'tone': 1041, 'tones': 1042, 'too': 1043, 'took': 1044, 'tottering': 1045, 'touched': 1046, 'toward': 1047, 'trace': 1048, 'trade': 1049, 'transmute': 1050, 'traps': 1051, 'travelled': 1052, 'tribute': 1053, 'tributes': 1054, 'tricks': 1055, 'tried': 1056, 'trouser-presses': 1057, 'true': 1058, 'truth': 1059, 'turned': 1060, 'twenty': 1061, 'twenty-four': 1062, 'twice': 1063, 'twirling': 1064, 'unaccountable': 1065, 'uncertain': 1066, 'under': 1067, 'underlay': 1068, 'underneath': 1069, 'understand': 1070, 'unexpected': 1071, 'untouched': 1072, 'unusual': 1073, 'up': 1074, 'up-stream': 1075, 'upon': 1076, 'upset': 1077, 'upstairs': 1078, 'us': 1079, 'used': 1080, 'usual': 1081, 'value': 1082, 'varnishing': 1083, 'vases': 1084, 'veins': 1085, 'velveteen': 1086, 'very': 1087, 'villa': 1088, 'vindicated': 1089, 'virtuosity': 1090, 'vista': 1091, 'vocation': 1092, 'voice': 1093, 'wall': 1094, 'wander': 1095, 'want': 1096, 'wanted': 1097, 'wants': 1098, 'was': 1099, \"wasn't\": 1100, 'watched': 1101, 'watching': 1102, 'water-colour': 1103, 'waves': 1104, 'way': 1105, 'weekly': 1106, 'weeks': 1107, 'welcome': 1108, 'went': 1109, 'were': 1110, 'what': 1111, 'when': 1112, 'whenever': 1113, 'where': 1114, 'which': 1115, 'while': 1116, 'white': 1117, 'white-panelled': 1118, 'who': 1119, 'whole': 1120, 'whom': 1121, 'why': 1122, 'wide': 1123, 'widow': 1124, 'wife': 1125, \"wife's\": 1126, 'wild': 1127, 'wincing': 1128, 'window-curtains': 1129, 'wish': 1130, 'with': 1131, 'without': 1132, 'wits': 1133, 'woman': 1134, 'women': 1135, \"won't\": 1136, 'wonder': 1137, 'wondered': 1138, 'word': 1139, 'work': 1140, 'working': 1141, 'worth': 1142, 'would': 1143, \"wouldn't\": 1144, 'year': 1145, 'years': 1146, 'yellow': 1147, 'yet': 1148, 'you': 1149, \"you'd\": 1150, \"you're\": 1151, 'younger': 1152, 'your': 1153, 'yourself': 1154, '<|endoftext|>': 1155, '<|unk|>': 1156}\n",
      "('younger', 1152)\n",
      "('your', 1153)\n",
      "('yourself', 1154)\n",
      "('<|endoftext|>', 1155)\n",
      "('<|unk|>', 1156)\n",
      "[1156, 4, 378, 1149, 653, 996, 9, 1155, 60, 1009, 978, 1005, 746, 1009, 1156, 6]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab=raw_text)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "total_text = \"<|endoftext|>\".join((text1, text2))\n",
    "\n",
    "ids = tokenizer.encode(text=total_text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d5e5b",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(249, 236, 228); font-size:20px\">Next, let’s see whether we can turn these token IDs back</br>\n",
    "into text using the decode method:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1d054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|> , do you like tea ? <|endoftext|> In the sunlit terraces of the <|unk|> .\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids=ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e482733",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">2.5 Byte pair encodin : \n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbed31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "330b4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6282f75",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(246, 226, 212); font-size:20px\">The usage of this tokenizer is similar to the SimpleTokenizerV2\n",
    "</br>\n",
    "we implemented previously via an encode method :</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4696ec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    " \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    " \"of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61ca57",
   "metadata": {},
   "source": [
    "<span style=\"color:rgb(246, 226, 212); font-size:20px\">convert the token IDs back into text using the\n",
    "</br>\n",
    "decode method, similar to our SimpleTokenizerV2:\n",
    " :</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d94b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b956f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "b'Ak'\n",
      "b'w'\n",
      "b'ir'\n",
      "b'w'\n",
      "b' '\n",
      "b'ier'\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n",
    "for ids in integers:\n",
    "    print(tokenizer.decode_single_token_bytes(ids))\n",
    "\n",
    "print(tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7f522",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\">2.6 Data sampling with a sliding\n",
    "window : \n",
    "</h1></br>\n",
    "<span style=\"color:rgb(246, 226, 212); font-size:20px\">generate the input–target pairs required for training an LLM</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1627b07",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "## 🧠 Sliding-Window و Batching در آموزش GPT-style Language Models\n",
    "\n",
    "فرض کنید جمله‌ی زیر رو داریم:\n",
    "\n",
    "بعد از تبدیل به توکن (مثلاً با BPE)، به شکل زیر درمیاد:\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Sliding Window — ساخت input و target\n",
    "\n",
    "فرض کنیم طول پنجره (window) برابر ۳ باشه (`max_length = 3`).  \n",
    "در این روش، از دنباله‌ی توکن‌ها، ورودی و خروجی‌هایی می‌سازیم که مدل بتونه توکن بعدی رو پیش‌بینی کنه:\n",
    "\n",
    "| Input               | Target              |\n",
    "|--------------------|---------------------|\n",
    "| [I, love, you]     | [love, you, so]     |\n",
    "| [love, you, so]    | [you, so, much]     |\n",
    "| [you, so, much]    | [so, much, baby]    |\n",
    "| [so, much, baby]   | [much, baby, kiss]  |\n",
    "| [much, baby, kiss] | [baby, kiss, me]    |\n",
    "| [baby, kiss, me]   | [kiss, me, please]  |\n",
    "\n",
    "در واقع، خروجی فقط یک توکن شیفت‌یافته از ورودی هست. هدف مدل اینه که برای هر موقعیت، توکن بعدی رو پیش‌بینی کنه.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Batching — آموزش موازی\n",
    "\n",
    "برای افزایش کارایی، این جفت‌ها رو با batch_size = 3 در قالب تنسور به مدل می‌دیم:\n",
    "\n",
    "#### 🔹 Batch 1:\n",
    "```python\n",
    "inputs  = [[I, love, you],\n",
    "           [love, you, so],\n",
    "           [you, so, much]]\n",
    "\n",
    "targets = [[love, you, so],\n",
    "           [you, so, much],\n",
    "           [so, much, baby]]\n",
    "\n",
    "inputs  = [[so, much, baby],\n",
    "           [much, baby, kiss],\n",
    "           [baby, kiss, me]]\n",
    "\n",
    "targets = [[much, baby, kiss],\n",
    "           [baby, kiss, me],\n",
    "           [kiss, me, please]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c36109ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "encoding_text = tokenizer.encode(raw_text)\n",
    "print(len(encoding_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2ab18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976, 13, 357, 10915, 314, 2138, 1807, 340, 561, 423, 587, 10598, 393, 28537, 2014, 198, 198, 1, 464, 6001, 286, 465, 13476, 1, 438, 5562, 373, 644, 262, 1466, 1444, 340, 13, 314, 460, 3285, 9074, 13, 46606, 536, 5469, 438, 14363, 938, 4842, 1650, 353, 438, 2934, 489, 3255, 465, 48422, 540, 450, 67, 3299, 13, 366, 5189, 1781, 340, 338, 1016, 284, 3758, 262, 1988, 286, 616, 4286, 705, 1014, 510, 26, 475, 314, 836, 470, 892, 286, 326, 11, 1770, 13, 8759, 2763, 438, 1169, 2994, 284, 943, 17034, 318, 477, 314, 892, 286, 526, 383, 1573, 11, 319, 9074, 13, 536, 5469, 338, 11914, 11, 33096, 663, 4808, 3808, 62, 355, 996, 484, 547, 12548, 287, 281, 13079, 410, 12523, 286, 22353, 13, 843, 340, 373, 407, 691, 262, 9074, 13, 536, 48819, 508, 25722, 276, 13, 11161, 407, 262, 40123, 18113, 544, 9325, 701, 11, 379, 262, 938, 402, 1617, 261, 12917, 905, 11, 5025, 502, 878, 402, 271, 10899, 338, 366, 31640, 12, 67, 20811, 1, 284, 910, 11, 351, 10953, 287, 607, 2951, 25, 366, 1135, 2236, 407, 804, 2402, 663, 588, 757, 13984, 198, 198, 5779, 28112, 10197, 832, 262, 46475, 286, 18113, 544, 338, 10953, 314, 2936, 1498, 284, 1986, 262, 1109, 351, 1602, 11227, 414, 13, 23676, 3619, 402, 271, 10899, 0, 383, 1466, 550, 925, 683, 438, 270, 373, 15830, 326, 484, 815, 25722, 683, 13, 9754, 465, 898, 1714, 7380, 30090, 547, 2982, 11, 290, 287, 465, 898, 3292, 8941, 257, 4636, 28582, 13, 18612, 35394, 30, 8673, 13, 1002, 340, 547, 11, 262, 15393, 286, 262, 5977, 373, 29178, 3474, 416, 1310, 40559, 11959, 1636, 11, 508, 11, 287, 477, 922, 4562, 11, 3181, 503, 287, 262, 37090, 257, 845, 22665, 366, 672, 270, 2838, 1, 319, 3619, 438, 505, 286, 883, 905, 88, 6685, 42070, 351, 4738, 6276, 871, 326, 314, 423, 2982, 357, 40, 1839, 470, 910, 416, 4150, 8, 3688, 284, 402, 271, 10899, 338, 12036, 13, 843, 523, 438, 14363, 10568, 852, 5729, 11331, 18893, 540, 438, 1169, 5114, 11835, 3724, 503, 11, 290, 11, 355, 9074, 13, 536, 5469, 550, 11001, 11, 262, 2756, 286, 366, 38, 271, 10899, 82, 1, 1816, 510, 13, 198, 198, 1026, 373, 407, 10597, 1115, 812, 1568, 326, 11, 287, 262, 1781, 286, 257, 1178, 2745, 6, 4686, 1359, 319, 262, 34686, 41976, 11, 340, 6451, 5091, 284, 502, 284, 4240, 1521, 402, 271, 10899, 550, 1813, 510, 465, 12036, 13, 1550, 14580, 11, 340, 1107, 373, 257, 29850, 1917, 13, 1675, 24456, 465, 3656, 561, 423, 587, 1165, 2562, 438, 14363, 3148, 1650, 1010, 550, 587, 6699, 262, 1540, 558, 286, 2282, 326, 9074, 13, 402, 271, 10899, 550, 366, 7109, 14655, 683, 866, 526, 1114, 9074, 13, 402, 271, 10899, 438, 292, 884, 438, 18108, 407, 11196, 10597, 3016, 257, 614, 706, 3619, 338, 10568, 550, 587, 2077, 13, 632, 1244, 307, 326, 339, 550, 6405, 607, 438, 20777, 339, 8288, 465, 10152, 438, 13893, 339, 1422, 470, 765, 284, 467, 319, 12036, 26, 475, 340, 561, 423, 587, 1327, 284, 5879, 326, 339, 550, 1813, 510, 465, 12036, 780, 339, 550, 6405, 607, 13, 198, 198, 5189, 1781, 11, 611, 673, 550, 407, 17901, 683, 866, 11, 673, 550, 8603, 11, 355, 4544, 9325, 701, 42397, 11, 4054, 284, 366, 26282, 683, 510, 1, 438, 7091, 550, 407, 2957, 683, 736, 284, 262, 1396, 417, 13, 1675, 1234, 262, 14093, 656, 465, 1021, 757, 438, 10919, 257, 410, 5040, 329, 257, 3656, 0, 887, 9074, 13, 402, 271, 10899, 4120, 284, 423, 595, 67, 1328, 340, 438, 392, 314, 2936, 340, 1244, 307, 3499, 284, 1064, 503, 1521, 13, 198, 198, 464, 748, 586, 652, 1204, 286, 262, 34686, 41976, 37733, 2346, 284, 884, 14177, 8233, 1020, 5768, 26, 290, 1719, 11, 319, 616, 835, 284, 22489, 40089, 11, 4978, 257, 19350, 286, 3619, 338, 3652, 436, 81, 5286, 8812, 2114, 1022, 262, 279, 1127, 11, 314, 550, 3589, 28068, 294, 1555, 262, 1306, 1110, 13, 198, 198, 40, 1043, 262, 3155, 379, 8887, 11061, 511, 18057, 12, 83, 6037, 26, 290, 9074, 13, 402, 271, 10899, 338, 7062, 373, 523, 2429, 498, 326, 11, 287, 262, 29543, 2745, 11, 314, 4752, 340, 6777, 13, 632, 373, 407, 326, 616, 2583, 408, 373, 366, 47914, 1298, 319, 326, 966, 314, 714, 423, 1813, 4544, 9325, 701, 262, 40830, 12719, 3874, 13, 632, 373, 655, 780, 673, 373, 4808, 1662, 62, 3499, 438, 361, 314, 743, 307, 41746, 12004, 262, 6473, 438, 5562, 314, 1043, 607, 523, 13, 1114, 3619, 11, 477, 465, 1204, 11, 550, 587, 11191, 416, 3499, 1466, 25, 484, 550, 26546, 1068, 465, 1242, 11, 340, 550, 587, 302, 1144, 287, 262, 3024, 12, 4803, 286, 511, 512, 1741, 13, 843, 340, 373, 4361, 5048, 425, 284, 3465, 644, 1245, 262, 366, 25124, 3101, 8137, 286, 16957, 1696, 414, 1, 357, 40, 9577, 4544, 9325, 701, 8, 373, 1719, 319, 683, 13, 198, 198, 40, 423, 4750, 326, 9074, 13, 402, 271, 10899, 373, 5527, 26, 290, 340, 373, 3393, 34953, 856, 326, 607, 5229, 373, 37895, 422, 428, 25179, 257, 19217, 475, 8904, 14676, 13, 632, 318, 11, 355, 257, 3896, 11, 262, 661, 508, 40987, 1637, 508, 651, 749, 503, 286, 340, 26, 290, 3619, 338, 19992, 31564, 286, 465, 3656, 338, 1263, 5236, 9343, 683, 11, 351, 281, 5585, 286, 2818, 922, 12, 49705, 11, 284, 21595, 1133, 340, 656, 5563, 286, 1242, 290, 13064, 13, 1675, 262, 6846, 11, 314, 1276, 751, 11, 339, 6150, 5365, 31655, 26, 475, 339, 373, 7067, 29396, 18443, 12271, 290, 45592, 12, 14792, 5986, 351, 257, 8839, 326, 7284, 35924, 262, 12306, 395, 4133, 13, 198, 198, 1, 26788, 338, 691, 12226, 318, 284, 1234, 8737, 656, 19133, 553, 373, 530, 286, 262, 7877, 72, 3150, 339, 8104, 866, 1973, 262, 37918, 411, 290, 8465, 286, 281, 33954, 271, 3973, 9899, 14678, 40556, 12, 11487, 11, 618, 11, 319, 257, 1568, 1110, 11, 314, 550, 757, 1057, 625, 422, 22489, 40089, 26, 290, 9074, 13, 402, 271, 10899, 11, 307, 3723, 319, 683, 11, 2087, 329, 616, 35957, 25, 366, 14295, 318, 523, 34813, 306, 8564, 284, 790, 1296, 286, 8737, 526, 198, 198, 43920, 3619, 0, 632, 550, 1464, 587, 465, 10030, 284, 423, 1466, 910, 884, 1243, 286, 683, 25, 262, 1109, 815, 307, 900, 866, 287, 1070, 268, 2288, 13, 1867, 7425, 502, 783, 373, 326, 11, 329, 262, 717, 640, 11, 339, 581, 4714, 262, 8216, 13, 314, 550, 1775, 683, 11, 523, 1690, 11, 1615, 3364, 739, 2092, 256, 7657, 438, 9776, 340, 262, 11644, 43778, 3465, 326, 26773, 606, 286, 511, 6799, 454, 30, 1400, 438, 1640, 11, 31414, 1576, 11, 340, 2627, 4156, 326, 339, 373, 16245, 286, 9074, 13, 402, 271, 10899, 438, 69, 623, 1576, 407, 284, 766, 607, 41793, 13, 632, 373, 465, 898, 41793, 339, 3947, 284, 307, 1592, 2259, 739, 438, 14363, 898, 9408, 355, 281, 2134, 329, 5482, 4447, 290, 753, 1072, 13, 198, 198, 1, 3666, 13674, 11, 1201, 314, 1053, 442, 17758, 12036, 661, 836, 470, 910, 326, 3404, 546, 502, 438, 9930, 910, 340, 546, 12622, 41379, 293, 553, 373, 465, 691, 5402, 11, 355, 339, 8278, 422, 262, 3084, 290, 336, 8375, 503, 4291, 262, 4252, 18250, 8812, 558, 13, 198, 198, 40, 27846, 706, 683, 11, 7425, 416, 465, 938, 1573, 13, 12622, 41379, 293, 373, 11, 287, 1109, 11, 5033, 262, 582, 286, 262, 2589, 438, 292, 3619, 2241, 11, 530, 1244, 1234, 340, 11, 550, 587, 262, 582, 286, 262, 1711, 13, 383, 7099, 6802, 373, 531, 284, 423, 7042, 2241, 379, 616, 1545, 338, 3625, 11, 290, 314, 14028, 611, 257, 256, 11912, 286, 35394, 739, 10724, 262, 6846, 338, 11428, 450, 67, 3299, 13, 887, 645, 438, 1640, 340, 373, 407, 10597, 706, 326, 1785, 326, 262, 4808, 13698, 10322, 6532, 62, 8263, 12, 9649, 550, 9258, 284, 3359, 511, 366, 8642, 521, 829, 526, 198, 198, 40, 2900, 284, 9074, 13, 402, 271, 10899, 11, 508, 550, 18459, 1068, 284, 1577, 257, 23844, 286, 7543, 284, 607, 599, 6321, 287, 262, 17423, 12, 3823, 13, 198, 198, 1, 5195, 4808, 10134, 62, 339, 442, 17758, 12036, 1701, 314, 1965, 25891, 13, 198, 198, 3347, 4376, 607, 26928, 351, 257, 9254, 286, 922, 12, 17047, 8167, 5975, 13, 198, 198, 1, 5812, 11, 339, 1595, 470, 4808, 14150, 62, 284, 783, 11, 345, 760, 26, 290, 314, 765, 683, 284, 2883, 2241, 553, 673, 531, 2407, 2391, 13, 198, 198, 40, 3114, 546, 262, 40894, 2330, 12, 6839, 11978, 2119, 11, 351, 663, 4808, 44769, 8270, 12, 332, 660, 62, 410, 1386, 20394, 262, 23755, 286, 262, 14005, 1801, 2093, 41160, 11, 290, 663, 45592, 12, 14792, 1613, 1424, 287, 19217, 24887, 13431, 13, 198, 198, 1, 19242, 339, 442, 17758, 465, 5986, 1165, 30, 314, 4398, 470, 1775, 257, 2060, 530, 287, 262, 2156, 526, 198, 198, 32, 3731, 17979, 286, 32315, 12606, 9074, 13, 402, 271, 10899, 338, 1280, 954, 36368, 13, 366, 1026, 338, 465, 11441, 48740, 11, 345, 760, 13, 679, 1139, 484, 821, 407, 4197, 284, 423, 546, 26, 339, 338, 1908, 606, 477, 1497, 2845, 530, 438, 1820, 18560, 438, 392, 326, 314, 423, 284, 1394, 26148, 526, 198, 198, 6653, 11441, 48740, 438, 14295, 338, 48740, 546, 465, 5986, 30, 2011, 20136, 373, 3957, 588, 262, 26394, 12, 301, 971, 13, 314, 531, 10722, 292, 2280, 284, 616, 2583, 408, 25, 366, 40, 1276, 1107, 766, 534, 18560, 11, 345, 760, 526, 198, 198, 3347, 27846, 503, 2048, 4628, 24882, 379, 262, 8812, 558, 810, 607, 5229, 11, 21081, 782, 278, 287, 257, 14263, 276, 5118, 11, 550, 6578, 257, 24518, 290, 7428, 262, 3394, 20096, 39047, 338, 1182, 1022, 465, 14475, 13, 198, 198, 1, 5779, 11, 1282, 981, 339, 338, 407, 2045, 553, 673, 531, 11, 351, 257, 6487, 326, 3088, 284, 7808, 607, 10927, 1108, 26, 290, 314, 3940, 607, 1022, 262, 30623, 2295, 49406, 286, 262, 6899, 11, 290, 510, 262, 3094, 16046, 351, 1059, 430, 12, 66, 12375, 299, 20896, 82, 24357, 1871, 12734, 379, 1123, 9581, 13, 198, 198, 818, 262, 5391, 76, 395, 5228, 286, 607, 275, 2778, 10840, 11, 10371, 257, 1534, 4241, 286, 19217, 290, 18876, 5563, 11, 9174, 530, 286, 262, 5385, 41186, 39614, 1386, 11, 287, 262, 13203, 5482, 1044, 276, 5739, 13, 383, 5019, 19001, 286, 262, 5739, 1444, 510, 477, 402, 271, 10899, 338, 1613, 0, 198, 198, 27034, 13, 402, 271, 10899, 9859, 736, 262, 4324, 12, 66, 3325, 1299, 11, 3888, 7263, 257, 4808, 73, 446, 259, 13235, 62, 1336, 286, 11398, 35560, 1000, 292, 11, 7121, 281, 3211, 12, 16337, 1497, 11, 290, 531, 25, 366, 1532, 345, 1302, 994, 345, 460, 655, 6687, 284, 766, 340, 13, 314, 550, 340, 625, 262, 24818, 417, 12, 12239, 11, 475, 339, 3636, 470, 1309, 340, 2652, 526, 198, 198, 5297, 438, 40, 714, 655, 6687, 284, 766, 340, 438, 1169, 717, 18560, 286, 3619, 338, 314, 550, 1683, 550, 284, 14022, 616, 2951, 625, 0, 19672, 484, 550, 262, 1295, 286, 15393, 438, 16706, 262, 4318, 6103, 287, 257, 14005, 7872, 393, 4808, 13698, 10322, 6532, 62, 8263, 12, 3823, 11, 393, 257, 36364, 1396, 417, 4624, 523, 326, 340, 1718, 262, 1657, 832, 41160, 286, 1468, 9932, 316, 666, 966, 13, 383, 517, 12949, 1295, 2627, 262, 4286, 1365, 26, 1865, 11, 355, 616, 2951, 6348, 23840, 284, 262, 2063, 12, 2971, 11, 477, 262, 16704, 14482, 1625, 503, 438, 439, 262, 10818, 20597, 32192, 355, 2709, 330, 871, 11, 262, 15910, 286, 16153, 312, 328, 3780, 416, 543, 11, 351, 884, 2784, 9830, 5032, 11, 339, 5257, 284, 36583, 3241, 422, 262, 1103, 1597, 286, 262, 4286, 284, 617, 2495, 11331, 2768, 590, 286, 3703, 13, 9074, 13, 402, 271, 10899, 11, 17728, 257, 8500, 4417, 284, 670, 319, 438, 15464, 11, 355, 340, 547, 11, 523, 16857, 262, 4469, 286, 607, 898, 4286, 438, 18108, 26269, 5223, 287, 281, 8468, 4922, 284, 262, 3359, 286, 428, 3991, 4118, 84, 16579, 13, 383, 4286, 373, 530, 286, 3619, 338, 366, 11576, 395, 553, 355, 465, 21099, 3808, 561, 423, 1234, 340, 438, 270, 7997, 11, 319, 465, 636, 11, 257, 29844, 286, 12749, 11, 257, 22791, 278, 286, 32375, 11, 257, 22486, 11, 965, 2860, 1359, 290, 965, 1397, 11, 326, 14516, 530, 286, 262, 33125, 12, 565, 593, 338, 25304, 4040, 284, 10303, 257, 17972, 13, 632, 1138, 11, 287, 1790, 11, 379, 790, 966, 262, 3512, 286, 14081, 2415, 284, 307, 13055, 366, 11576, 306, 1, 780, 673, 373, 10032, 286, 852, 13055, 366, 34751, 306, 1, 438, 392, 1865, 407, 284, 4425, 281, 22037, 286, 262, 32073, 13, 198, 198, 1, 1026, 338, 262, 938, 339, 13055, 11, 345, 760, 553, 9074, 13, 402, 271, 10899, 531, 351, 27322, 540, 11293, 13, 366, 464, 938, 475, 530, 553, 673, 19267, 5223, 438, 1, 4360, 262, 584, 1595, 470, 954, 11, 780, 339, 6572, 340, 526, 198, 198, 1, 49174, 276, 340, 1701, 314, 373, 546, 284, 1061, 510, 428, 18437, 618, 314, 2982, 257, 2366, 9662, 290, 2497, 3619, 2241, 319, 262, 11387, 13, 198, 198, 1722, 339, 6204, 612, 11, 465, 2832, 287, 262, 16511, 286, 465, 11555, 303, 7821, 13209, 11, 262, 7888, 7586, 9813, 286, 4190, 7121, 736, 422, 465, 2330, 22645, 11, 465, 10904, 4252, 6236, 429, 25839, 9230, 808, 276, 416, 257, 8212, 326, 13663, 262, 9040, 286, 257, 2116, 12, 10414, 738, 285, 23968, 4891, 11, 314, 2936, 284, 644, 257, 4922, 339, 550, 262, 976, 3081, 355, 465, 5986, 438, 1169, 3081, 286, 2045, 1190, 4119, 81, 621, 339, 373, 13, 198, 198, 6653, 3656, 27846, 379, 683, 1207, 8344, 803, 306, 11, 475, 465, 2951, 21650, 1613, 607, 284, 262, 18560, 13, 198, 198, 1, 5246, 13, 8759, 2763, 2227, 284, 766, 340, 553, 673, 2540, 11, 355, 611, 2859, 3500, 5223, 13, 679, 28271, 465, 12450, 11, 991, 16755, 13, 198, 198, 1, 5812, 11, 8759, 2763, 1043, 502, 503, 890, 2084, 553, 339, 531, 15376, 26, 788, 11, 6427, 465, 3211, 832, 6164, 25, 366, 16773, 290, 766, 262, 1334, 286, 262, 2156, 526, 198, 198, 1544, 3751, 340, 284, 502, 351, 257, 1611, 286, 24354, 20154, 11293, 25, 262, 7837, 12, 9649, 11, 262, 5486, 12, 83, 29080, 11, 262, 6576, 12, 565, 418, 1039, 11, 262, 4057, 2655, 12, 8439, 274, 438, 439, 262, 3716, 7106, 6637, 286, 262, 45172, 338, 5928, 3773, 13, 843, 8797, 616, 4240, 3432, 262, 2938, 17547, 339, 531, 11, 9644, 503, 465, 7721, 257, 1310, 25, 366, 5297, 11, 314, 1107, 836, 470, 766, 703, 661, 6687, 284, 2107, 1231, 326, 526, 198, 198, 5779, 438, 270, 373, 655, 262, 886, 530, 1244, 423, 1674, 15898, 329, 683, 13, 5514, 339, 373, 11, 832, 340, 477, 290, 287, 15275, 286, 340, 477, 438, 292, 339, 550, 587, 832, 11, 290, 287, 15275, 286, 11, 465, 5986, 438, 568, 22665, 11, 523, 23332, 11, 523, 595, 18052, 11, 326, 530, 890, 276, 284, 3960, 503, 25, 366, 3856, 44455, 351, 534, 24638, 2474, 355, 1752, 530, 550, 890, 276, 284, 910, 25, 366, 3856, 44455, 351, 534, 670, 2474, 198, 198, 1537, 11, 351, 262, 3960, 319, 616, 11914, 11, 616, 13669, 6989, 281, 10059, 2198, 13, 198, 198, 1, 1212, 318, 616, 898, 49451, 553, 339, 531, 11, 3756, 502, 656, 257, 3223, 8631, 2119, 379, 262, 886, 286, 262, 781, 273, 312, 410, 12523, 13, 632, 373, 6616, 290, 7586, 290, 11620, 88, 25, 645, 366, 34435, 8172, 645, 865, 291, 12, 64, 12, 1671, 330, 11, 4844, 286, 262, 1633, 286, 24380, 329, 20728, 287, 257, 4286, 10273, 438, 29370, 477, 11, 645, 1551, 1051, 286, 1683, 1719, 587, 973, 355, 257, 8034, 13, 198, 198, 464, 1109, 3181, 1363, 284, 502, 262, 4112, 957, 1483, 286, 3619, 338, 2270, 351, 465, 1468, 1204, 13, 198, 198, 1, 3987, 470, 345, 1683, 45553, 903, 351, 7521, 597, 517, 1701, 314, 1965, 11, 991, 2045, 546, 329, 257, 12854, 286, 884, 3842, 13, 198, 198, 1, 12295, 553, 339, 531, 11589, 13, 198, 198, 1, 5574, 1660, 12, 49903, 438, 273, 2123, 10813, 1701, 198, 198, 6653, 6563, 2951, 6348, 5391, 11, 290, 465, 25839, 279, 3021, 257, 1310, 739, 511, 22665, 4252, 10899, 13, 198, 198, 1, 12295, 892, 286, 340, 11, 616, 13674, 5891, 438, 1092, 517, 621, 611, 314, 1549, 1239, 12615, 257, 14093, 526, 198, 198, 1870, 465, 8216, 1297, 502, 287, 257, 7644, 326, 339, 1239, 1807, 286, 1997, 2073, 13, 198, 198, 40, 3888, 1497, 11, 43045, 21100, 416, 616, 10059, 9412, 26, 290, 355, 314, 2900, 11, 616, 4151, 3214, 319, 257, 1402, 4286, 2029, 262, 24818, 417, 12, 12239, 438, 1169, 691, 2134, 7163, 262, 8631, 26210, 3425, 9417, 286, 262, 2119, 13, 198, 198, 1, 5812, 11, 416, 449, 659, 2474, 314, 531, 13, 198, 198, 1026, 373, 257, 17548, 286, 257, 50085, 438, 272, 1468, 10032, 50085, 11, 5055, 287, 262, 6290, 739, 257, 3355, 13, 198, 198, 1, 3886, 449, 659, 438, 64, 520, 5493, 2474, 314, 16896, 13, 198, 198, 1544, 373, 10574, 26, 475, 314, 2936, 683, 1969, 2157, 502, 11, 12704, 257, 1310, 2952, 13, 198, 198, 1, 2061, 257, 4240, 0, 14446, 351, 257, 8667, 3951, 438, 4360, 319, 45697, 19369, 13, 921, 9670, 28022, 11, 810, 750, 345, 651, 340, 1701, 198, 198, 1544, 9373, 6364, 25, 366, 27034, 13, 520, 5493, 2921, 340, 284, 502, 526, 198, 198, 1, 10910, 438, 40, 1422, 470, 760, 345, 772, 2993, 262, 520, 5493, 82, 13, 679, 373, 884, 281, 1167, 2588, 856, 607, 2781, 526, 198, 198, 1, 40, 1422, 470, 438, 83, 359, 706, 13, 764, 764, 764, 1375, 1908, 329, 502, 284, 7521, 683, 618, 339, 373, 2636, 526, 198, 198, 1, 2215, 339, 373, 2636, 30, 921, 1701, 198, 198, 40, 1276, 423, 1309, 257, 1310, 1165, 881, 40642, 972, 6654, 832, 616, 5975, 11, 329, 339, 9373, 351, 257, 1207, 8344, 803, 6487, 25, 366, 5297, 438, 7091, 338, 281, 12659, 2829, 1122, 11, 345, 760, 11, 9074, 13, 520, 5493, 13, 2332, 691, 2126, 373, 284, 423, 683, 1760, 416, 257, 38378, 34537, 438, 993, 11, 3595, 520, 5493, 0, 1375, 1807, 340, 262, 1654, 301, 835, 286, 46431, 465, 27951, 438, 1659, 10833, 340, 319, 257, 1308, 27461, 1171, 13, 843, 379, 262, 2589, 314, 373, 4808, 1169, 62, 38378, 34537, 526, 198, 198, 1, 10910, 11, 3595, 520, 5493, 438, 292, 345, 910, 13, 8920, 4808, 5562, 62, 465, 2106, 1701, 198, 198, 1, 2504, 373, 465, 2106, 13, 1375, 4762, 287, 683, 11, 26996, 798, 287, 683, 438, 273, 1807, 673, 750, 13, 887, 673, 3521, 470, 6842, 407, 284, 423, 477, 262, 8263, 12, 9649, 351, 607, 13, 1375, 3521, 470, 6842, 262, 1109, 326, 11, 319, 1401, 77, 3929, 1528, 11, 530, 714, 1464, 651, 1474, 1576, 284, 766, 465, 5986, 13, 23676, 2415, 0, 1375, 338, 655, 257, 24225, 39136, 278, 329, 584, 21441, 13, 520, 5493, 318, 262, 691, 2187, 314, 1683, 2993, 526, 198, 198, 1, 1639, 1683, 2993, 30, 887, 345, 655, 531, 438, 1, 198, 198, 38, 271, 10899, 550, 257, 11040, 8212, 287, 465, 2951, 13, 198, 198, 1, 5812, 11, 314, 2993, 683, 11, 290, 339, 2993, 502, 438, 8807, 340, 3022, 706, 339, 373, 2636, 526, 198, 198, 40, 5710, 616, 3809, 43045, 13, 366, 2215, 673, 1908, 329, 345, 1701, 198, 198, 1, 5297, 438, 37121, 1035, 27339, 284, 262, 21296, 13, 1375, 2227, 683, 29178, 3474, 438, 392, 416, 502, 2474, 198, 198, 1544, 13818, 757, 11, 290, 9617, 736, 465, 1182, 284, 804, 510, 379, 262, 17548, 286, 262, 50085, 13, 366, 1858, 547, 1528, 618, 314, 3521, 470, 804, 379, 326, 1517, 438, 24089, 77, 470, 1986, 340, 13, 887, 314, 4137, 3589, 284, 1234, 340, 994, 26, 290, 783, 340, 338, 30703, 502, 438, 66, 1522, 502, 13, 1320, 338, 262, 1738, 1521, 314, 836, 470, 45553, 903, 597, 517, 11, 616, 13674, 8759, 2763, 26, 393, 2138, 520, 5493, 2241, 318, 262, 1738, 526, 198, 198, 1890, 262, 717, 640, 616, 21696, 20136, 546, 616, 15185, 2900, 656, 257, 2726, 6227, 284, 1833, 683, 1365, 13, 198, 198, 1, 40, 4601, 345, 1549, 1560, 502, 703, 340, 3022, 553, 314, 531, 13, 198, 198, 1544, 6204, 2045, 510, 379, 262, 17548, 11, 290, 665, 24297, 1022, 465, 9353, 257, 17779, 339, 550, 11564, 284, 1657, 13, 24975, 339, 2900, 3812, 502, 13, 198, 198, 1, 40, 1549, 2138, 588, 284, 1560, 345, 438, 13893, 314, 1053, 1464, 9885, 345, 286, 2376, 26927, 616, 670, 526, 198, 198, 40, 925, 257, 1207, 8344, 803, 18342, 11, 543, 339, 2469, 265, 1572, 351, 257, 922, 12, 17047, 8167, 32545, 13, 198, 198, 1, 5812, 11, 314, 1422, 470, 1337, 257, 14787, 618, 314, 4762, 287, 3589, 438, 392, 783, 340, 338, 281, 2087, 9839, 1022, 514, 2474, 198, 198, 1544, 13818, 4622, 11, 1231, 35987, 11, 290, 7121, 530, 286, 262, 2769, 3211, 12, 49655, 2651, 13, 366, 1858, 25, 787, 3511, 6792, 438, 392, 994, 389, 262, 33204, 345, 588, 526, 198, 198, 1544, 4624, 606, 379, 616, 22662, 290, 3767, 284, 27776, 510, 290, 866, 262, 2119, 11, 12225, 783, 290, 788, 11061, 262, 4286, 13, 198, 198, 1, 2437, 340, 3022, 30, 314, 460, 1560, 345, 287, 1936, 2431, 438, 392, 340, 1422, 470, 1011, 881, 2392, 284, 1645, 13, 764, 764, 764, 314, 460, 3505, 783, 703, 6655, 290, 10607, 314, 373, 618, 314, 1392, 9074, 13, 520, 5493, 338, 3465, 13, 3226, 1781, 11, 2769, 866, 11, 314, 550, 1464, 4808, 31985, 62, 612, 373, 645, 530, 588, 683, 438, 8807, 314, 550, 3750, 351, 262, 4269, 11, 22211, 262, 6678, 40315, 10455, 546, 683, 11, 10597, 314, 2063, 1392, 284, 892, 339, 373, 257, 5287, 11, 530, 286, 262, 1611, 326, 389, 1364, 2157, 13, 2750, 449, 659, 11, 290, 339, 4808, 9776, 62, 1364, 2157, 438, 13893, 339, 550, 1282, 284, 2652, 0, 383, 1334, 286, 514, 550, 284, 1309, 6731, 307, 17676, 1863, 393, 467, 739, 11, 475, 339, 373, 1029, 2029, 262, 1459, 438, 261, 45697, 19369, 11, 355, 345, 910, 13, 198, 198, 1, 5779, 11, 314, 1816, 572, 284, 262, 2156, 287, 616, 749, 34372, 10038, 438, 34330, 3888, 11, 4453, 20927, 502, 11, 379, 262, 3108, 418, 286, 3595, 520, 5493, 338, 3451, 286, 5287, 852, 37492, 416, 262, 13476, 286, 616, 12036, 683, 0, 3226, 1781, 314, 4001, 284, 466, 262, 4286, 329, 2147, 438, 40, 1297, 9074, 13, 520, 5493, 523, 618, 673, 2540, 284, 336, 321, 647, 1223, 546, 607, 8098, 13, 314, 3505, 1972, 572, 257, 40426, 10956, 9546, 546, 262, 15393, 852, 4808, 3810, 62, 438, 1219, 11, 314, 373, 19716, 306, 11, 616, 13674, 8759, 2763, 0, 314, 373, 24380, 284, 3589, 588, 530, 286, 616, 898, 1650, 1010, 13, 198, 198, 1, 6423, 314, 373, 2077, 510, 290, 1364, 3436, 351, 683, 13, 314, 550, 1908, 477, 616, 20348, 287, 5963, 11, 290, 314, 550, 691, 284, 900, 510, 262, 1396, 417, 290, 651, 284, 670, 13, 679, 550, 587, 2636, 691, 8208, 12, 14337, 2250, 11, 290, 339, 3724, 6451, 11, 286, 2612, 4369, 11, 523, 326, 612, 550, 587, 645, 15223, 670, 286, 8166, 438, 14363, 1986, 373, 1598, 290, 36519, 13, 314, 550, 1138, 683, 1752, 393, 5403, 11, 812, 878, 11, 290, 1807, 683, 32081, 290, 44852, 88, 13, 2735, 314, 2497, 326, 339, 373, 21840, 13, 198, 198, 1, 40, 373, 9675, 379, 717, 11, 351, 257, 6974, 19713, 14676, 25, 9675, 284, 423, 616, 1021, 319, 884, 257, 705, 32796, 2637, 3244, 465, 6283, 1204, 12, 46965, 9449, 2540, 284, 2689, 502, 24506, 306, 438, 292, 314, 10226, 262, 1182, 287, 314, 2936, 355, 611, 339, 547, 4964, 502, 466, 340, 13, 383, 18098, 373, 3940, 416, 262, 1807, 25, 611, 339, 4808, 22474, 62, 4964, 502, 11, 644, 561, 339, 910, 284, 616, 835, 286, 1762, 30, 2011, 29483, 2540, 284, 467, 257, 1310, 4295, 438, 40, 2936, 10927, 290, 8627, 13, 198, 198, 1, 7454, 11, 618, 314, 3114, 510, 11, 314, 3947, 284, 766, 257, 8212, 2157, 465, 1969, 12768, 680, 21213, 438, 292, 611, 339, 550, 262, 3200, 11, 290, 547, 28297, 2241, 416, 4769, 340, 736, 422, 502, 13, 1320, 41851, 515, 502, 991, 517, 13, 383, 3200, 30, 4162, 11, 314, 550, 257, 3200, 2861, 8208, 286, 465, 0, 314, 37901, 379, 262, 21978, 44896, 11, 290, 3088, 617, 286, 616, 49025, 5330, 15910, 13, 887, 484, 4054, 502, 11, 484, 1067, 11137, 13, 314, 2497, 326, 339, 2492, 470, 4964, 262, 905, 88, 10340, 438, 40, 3521, 470, 11786, 465, 3241, 26, 339, 655, 4030, 465, 2951, 319, 262, 1327, 22674, 1022, 13, 5845, 547, 262, 3392, 314, 550, 1464, 427, 343, 9091, 11, 393, 5017, 510, 351, 617, 9105, 7521, 13, 843, 703, 339, 2497, 832, 616, 7363, 0, 198, 198, 1, 40, 3114, 510, 757, 11, 290, 4978, 6504, 286, 326, 17548, 286, 262, 50085, 10938, 319, 262, 3355, 1474, 465, 3996, 13, 2399, 3656, 1297, 502, 20875, 340, 373, 262, 938, 1517, 339, 550, 1760, 438, 3137, 257, 3465, 2077, 351, 257, 17275, 1021, 11, 618, 339, 373, 866, 287, 6245, 684, 10695, 20222, 422, 257, 2180, 2612, 1368, 13, 2329, 257, 3465, 0, 887, 340, 4952, 465, 2187, 2106, 13, 1318, 389, 812, 286, 5827, 40987, 913, 30802, 287, 790, 1627, 13, 317, 582, 508, 550, 1509, 388, 351, 262, 1459, 714, 1239, 423, 4499, 326, 18680, 510, 12, 5532, 14000, 13, 764, 764, 764, 198, 198, 1, 40, 2900, 736, 284, 616, 670, 11, 290, 1816, 319, 39136, 278, 290, 285, 4185, 1359, 26, 788, 314, 3114, 379, 262, 50085, 757, 13, 314, 2497, 326, 11, 618, 520, 5493, 8104, 287, 262, 717, 14000, 11, 339, 2993, 655, 644, 262, 886, 561, 307, 13, 679, 550, 17273, 465, 2426, 11, 19233, 340, 11, 11027, 515, 340, 13, 1649, 550, 314, 1760, 326, 351, 597, 286, 616, 1243, 30, 1119, 8020, 470, 587, 4642, 286, 502, 438, 40, 550, 655, 8197, 606, 13, 764, 764, 764, 198, 198, 1, 39, 648, 340, 11, 8759, 2763, 11, 351, 326, 1986, 4964, 502, 314, 3521, 470, 466, 1194, 14000, 13, 383, 8631, 3872, 373, 11, 314, 1422, 470, 760, 810, 284, 1234, 340, 438, 62, 40, 550, 1239, 1900, 44807, 5514, 11, 351, 616, 1650, 1010, 290, 616, 1171, 11, 257, 905, 88, 22870, 286, 9568, 5017, 510, 262, 1109, 438, 40, 655, 9617, 7521, 656, 511, 6698, 13, 764, 764, 764, 3894, 11, 7521, 373, 262, 530, 7090, 883, 2636, 2951, 714, 766, 832, 438, 3826, 3892, 284, 262, 2006, 20212, 19369, 14638, 13, 2094, 470, 345, 760, 703, 11, 287, 3375, 257, 3215, 3303, 11, 772, 6562, 1473, 11, 530, 1139, 2063, 262, 640, 407, 644, 530, 3382, 284, 475, 644, 530, 460, 30, 3894, 438, 5562, 373, 262, 835, 314, 13055, 26, 290, 355, 339, 3830, 612, 290, 7342, 502, 11, 262, 1517, 484, 1444, 616, 705, 23873, 2350, 6, 14707, 588, 257, 2156, 286, 4116, 13, 679, 1422, 470, 10505, 263, 11, 345, 1833, 11, 3595, 520, 5493, 438, 258, 655, 3830, 612, 12703, 4964, 11, 290, 319, 465, 11914, 11, 832, 262, 12768, 21213, 11, 314, 3947, 284, 3285, 262, 1808, 25, 705, 8491, 345, 1654, 345, 760, 810, 345, 821, 2406, 503, 8348, 198, 198, 1, 1532, 314, 714, 423, 13055, 326, 1986, 11, 351, 326, 1808, 319, 340, 11, 314, 815, 423, 1760, 257, 1049, 1517, 13, 383, 1306, 6000, 1517, 373, 284, 766, 326, 314, 3521, 470, 438, 392, 326, 11542, 373, 1813, 502, 13, 887, 11, 11752, 11, 379, 326, 5664, 11, 8759, 2763, 11, 373, 612, 1997, 319, 4534, 314, 3636, 470, 423, 1813, 284, 423, 520, 5493, 6776, 878, 502, 11, 290, 284, 3285, 683, 910, 25, 705, 1026, 338, 407, 1165, 2739, 438, 40, 1183, 905, 345, 703, 30960, 198, 198, 1, 1026, 4808, 9776, 62, 1165, 2739, 438, 270, 561, 423, 587, 11, 772, 611, 339, 1549, 587, 6776, 13, 314, 11856, 510, 616, 20348, 11, 290, 1816, 866, 290, 1297, 9074, 13, 520, 5493, 13, 3226, 1781, 314, 1422, 470, 1560, 607, 4808, 5562, 62, 438, 270, 561, 423, 587, 8312, 284, 607, 13, 314, 2391, 531, 314, 3521, 470, 7521, 683, 11, 326, 314, 373, 1165, 3888, 13, 1375, 2138, 8288, 262, 2126, 438, 7091, 338, 523, 14348, 0, 632, 373, 326, 326, 925, 607, 1577, 502, 262, 50085, 13, 887, 673, 373, 22121, 9247, 379, 407, 1972, 262, 18560, 438, 7091, 750, 523, 765, 683, 705, 28060, 6, 416, 617, 530, 905, 88, 0, 1629, 717, 314, 373, 7787, 673, 3636, 470, 1309, 502, 572, 438, 392, 379, 616, 266, 896, 6, 886, 314, 5220, 41379, 293, 13, 3363, 11, 340, 373, 314, 508, 2067, 41379, 293, 25, 314, 1297, 9074, 13, 520, 5493, 339, 373, 262, 705, 4976, 6, 582, 11, 290, 673, 1297, 8276, 2073, 11, 290, 523, 340, 1392, 284, 307, 2081, 13, 764, 764, 764, 843, 339, 13055, 520, 5493, 1231, 1592, 2259, 26, 290, 673, 9174, 262, 4286, 1871, 607, 5229, 338, 1243, 13, 764, 764, 22135, 198, 198, 1544, 45111, 2241, 866, 287, 262, 3211, 12, 16337, 1474, 6164, 11, 8104, 736, 465, 1182, 11, 290, 47425, 278, 465, 5101, 11061, 340, 11, 3114, 510, 379, 262, 4286, 2029, 262, 18205, 1681, 12, 12239, 13, 198, 198, 1, 40, 588, 284, 14996, 326, 520, 5493, 2241, 561, 423, 1813, 340, 284, 502, 11, 611, 339, 1549, 587, 1498, 284, 910, 644, 339, 1807, 326, 1110, 526, 198, 198, 1870, 11, 287, 3280, 284, 257, 1808, 314, 1234, 2063, 12, 1326, 3147, 1146, 438, 1, 44140, 757, 1701, 339, 30050, 503, 13, 366, 2215, 262, 530, 1517, 326, 6774, 502, 6609, 1474, 683, 318, 326, 314, 2993, 1576, 284, 2666, 572, 1701, 198, 198, 1544, 6204, 510, 290, 8104, 465, 1021, 319, 616, 8163, 351, 257, 6487, 13, 366, 10049, 262, 21296, 286, 340, 318, 326, 314, 4808, 321, 62, 991, 12036, 438, 20777, 41379, 293, 338, 1804, 340, 329, 502, 0, 383, 520, 5493, 82, 1302, 3436, 11, 290, 1645, 1752, 438, 4360, 612, 338, 645, 42393, 803, 674, 1611, 286, 1242, 526]\n"
     ]
    }
   ],
   "source": [
    "enc_sample = encoding_text[50:]\n",
    "print(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90c28ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"X:  {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20870a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ---------> 4920\n",
      " and --------->  established\n",
      "[290, 4920] ---------> 2241\n",
      " and established --------->  himself\n",
      "[290, 4920, 2241] ---------> 287\n",
      " and established himself --------->  in\n",
      "[290, 4920, 2241, 287] ---------> 257\n",
      " and established himself in --------->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"--------->\", desired)\n",
    "    print(tokenizer.decode(context), \"--------->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f219b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(253, 173, 119);\"> Dataset for Data sampling with a sliding : \n",
    "</h1></br>\n",
    "The GPTDatasetV1 class is based on the PyTorch Dataset class\n",
    "and defines how individual</br> rows are fetched from the\n",
    "dataset, where each row consists of a number of token IDs</br>\n",
    "(based on a max_length) assigned to an input_chunk tensor. The\n",
    "target_ chunk tensor</br> contains the corresponding targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89719cd3",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "\n",
    "## 📄 کلاس GPTDatasetV1 — آماده‌سازی داده‌ی آموزشی برای GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48bf5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids  = []        # همه‌ی توالی‌های ورودی (X)\n",
    "        self.target_ids = []        # همه‌ی توالی‌های هدف   (Y)\n",
    "\n",
    "        # 1️⃣  کل متن را توکنیزه می‌کنیم\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # 2️⃣  با Sliding-Window متن را به سکانس‌های هم‌پوشان تقسیم می‌کنیم\n",
    "        #     طول هر سکانس = max_length\n",
    "        #     هر بار به‌اندازه‌ی  stride  جلو می‌رویم\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk  = token_ids[i            : i + max_length]\n",
    "            target_chunk = token_ids[i + stride        : i + max_length + stride]  # یک توکن شیفت جلوتر\n",
    "\n",
    "            # 3️⃣  تبدیل هر قطعه به Tensor برای PyTorch\n",
    "            self.input_ids .append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # 4️⃣  تعداد کل نمونه‌ها (برای DataLoader)\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # 5️⃣  برگرداندن یک جفت (X, Y) بر اساس اندیس\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e9f317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(\n",
    "        txt: str,\n",
    "        batch_size: int = 4,\n",
    "        max_length: int = 256,\n",
    "        stride: int = 128,\n",
    "        shuffle: bool = True,\n",
    "        drop_last: bool = True,\n",
    "        num_workers: int = 0):\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # 1) ساخت توکنایزر BPE (tikToken = سریع و سبک)\n",
    "    # -------------------------------------------\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")  # می‌توان مدل دلخواه BPE را انتخاب کرد\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 2) تبدیل کل متن به دیتاست Sliding-Window (GPTDatasetV1)\n",
    "    # ----------------------------------------------------------\n",
    "    dataset = GPTDatasetV1(\n",
    "        txt=txt,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        stride=stride)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 3) ساخت DataLoader رسمی PyTorch\n",
    "    #    - drop_last : جلوگیری از بتچ کوچک\n",
    "    #    - shuffle   : برای آموزش بهتر\n",
    "    #    - num_workers: سرعت بیشتر هنگام CPU-preprocessing\n",
    "    # ----------------------------------------------------------\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # برگشت آبجکت آمادهٔ مصرف توسط حلقهٔ آموزش\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03172caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#iter example for test\n",
    "itera = [1, 2, 3]\n",
    "my_iter = iter(itera)\n",
    "data = next(my_iter)\n",
    "print(data)\n",
    "data = next(my_iter)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a88e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  40,  367, 2885, 1464]])\n",
      "\n",
      "Targets:\\n tensor([[ 367, 2885, 1464, 1807]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    " raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "#1 Converts dataloader into a Python iterator to fetch the next entry via|>\n",
    "#|>Python’s built-in next() function\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f8e15",
   "metadata": {},
   "source": [
    "## 2.7 Creating Token Embeddings\n",
    "\n",
    "در این بخش، هدف ما ایجاد embedding برای توکن‌های عددی است که نماینده‌ی کلمات یا نشانه‌ها در یک مدل زبان هستند.\n",
    "\n",
    "### 📌 مرحله 1: تعریف ورودی\n",
    "\n",
    "```python\n",
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42ea3727",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bc6cc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4d90834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3aff81",
   "metadata": {},
   "source": [
    "## 2.8 Encoding Word Positions 🧭\n",
    "\n",
    "### 🧠 مشکل اصلی:\n",
    "در مدل‌های زبان بزرگ (LLMs)، لایه‌ی embedding توکن‌ها را به بردارهایی نگاشت می‌کند. اما یک نکته‌ی مهم اینجاست:\n",
    "\n",
    "> ❗ Self-attention به‌تنهایی ترتیب یا مکان توکن‌ها را نمی‌فهمد!\n",
    "\n",
    "مثلاً توکن ۵ همیشه به یک بردار مشخص نگاشت می‌شود، بدون توجه به این‌که در اول جمله آمده یا آخر.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 رفتار لایه‌ی embedding\n",
    "\n",
    "لایه‌ی embedding به‌صورت زیر عمل می‌کند:\n",
    "\n",
    "```text\n",
    "Token IDs = [2, 3, 5, 2]  # معادل جمله‌ی فرضی \"fox jumps over fox\"\n",
    "\n",
    "Embedding Weight Matrix:\n",
    "ID 2 → [1.2753, -0.2010, -0.1606]\n",
    "ID 3 → [0.4015,  0.9666, -1.1481]\n",
    "ID 5 → [2.8400, -0.7849, -1.4096]\n",
    "\n",
    "[1.2753, -0.2010, -0.1606]  ← fox (اول)\n",
    "[0.4015,  0.9666, -1.1481]  ← jumps\n",
    "[2.8400, -0.7849, -1.4096]  ← over\n",
    "[1.2753, -0.2010, -0.1606]  ← fox (دوباره!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7902c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8448aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\\n torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, \n",
    "    max_length=max_length, \n",
    "    stride=max_length, \n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef4ecee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "print(token_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586a18b",
   "metadata": {},
   "source": [
    "### 📍 Position Embedding در PyTorch\n",
    "\n",
    "در معماری Transformer**، چون هیچ ترتیب ذاتی‌ای بین توکن‌ها وجود ندارد، لازم است **موقعیت (Position) هر توکن به مدل داده شود تا بتواند ترتیب کلمات را درک کند. این کار از طریق Position Embedding انجام می‌شود.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠️ ساخت Position Embedding\n",
    "\n",
    "در این مثال ساده، مراحل زیر انجام می‌شود:\n",
    "\n",
    "- 🔢 context_length برابر است با حداکثر تعداد توکن‌هایی که مدل در یک ورودی می‌بیند.\n",
    "- 🧱 یک لایه‌ی Embedding به‌صورت context_length × output_dim ساخته می‌شود.\n",
    "  - این لایه به هر موقعیت از 0 تا context_length - 1 یک بردار ویژگی (embedding) اختصاص می‌دهد.\n",
    "- 📊 با استفاده از torch.arange(context_length) یک آرایه از موقعیت‌ها ساخته می‌شود (مثلاً `[0, 1, 2, 3]`).\n",
    "- 🧬 این آرایه به لایه‌ی embedding داده می‌شود تا بردار embedding مربوط به هر موقعیت تولید شود.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ خروجی نمونه:\n",
    "\n",
    "```python\n",
    "torch.Size([4, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d10c109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb6d811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embedding + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d0140ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7120, 7002, 4940, 351, 530, 2239, 220]\n",
      "tensor([[7120, 7002, 4940,  351,  530, 2239,  220]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Your journey starts with one step \"\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "\n",
    "integers_torch = torch.tensor(integers).unsqueeze(0)\n",
    "print(integers_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc379d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2533, -0.0438, -0.8669,  ...,  0.6702,  0.2240, -0.5718],\n",
      "         [ 0.0643, -0.3555,  0.0845,  ..., -0.1261, -0.4285, -0.1846],\n",
      "         [ 0.8342,  0.5134,  0.6161,  ..., -1.2478, -0.3091, -0.4559],\n",
      "         ...,\n",
      "         [-0.6179,  0.2314, -0.5075,  ..., -0.4943,  0.2457,  0.3168],\n",
      "         [-0.8742,  0.9597, -0.4078,  ...,  1.4336, -0.6442,  1.0769],\n",
      "         [-1.4157, -0.2557,  1.6429,  ..., -1.0009, -0.8753, -0.5352]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(token_embedding_layer(integers_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f381e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "💡 چرا با وجود داشتن شناسه‌های واژه، باز هم به برش پنجره‌ای نیاز داریم؟\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 تفاوت بین سه مرحلهٔ اصلی در مدل زبانی\n",
    "\n",
    "| مرحله | هدف | خروجی |\n",
    "|-------|------|--------|\n",
    "| رمزنگاری واژه‌ها | تبدیل واژه‌ها به اعداد | برداری از اعداد مانند [۱۲، ۳۱، ۴۶۷، ...] |\n",
    "| تعبیهٔ برداری | نگاشت هر عدد به یک بردار چگال | آرایه‌ای سه‌بعدی از بردارهای معنا‌دار |\n",
    "| آماده‌سازی داده‌ها | ساخت داده‌های آموزشی | مجموعه‌ای از ورودی و خروجی برای مدل |\n",
    "\n",
    "🔸 مرحلهٔ سوم، همان جایی‌ست که «برش پنجره‌ای» انجام می‌شود.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 دلایل نیاز به برش پنجره‌ای\n",
    "\n",
    "۱. محدودیت حافظه  \n",
    "مدل نمی‌تواند یک متن بسیار طولانی را به‌صورت یک‌جا پردازش کند. پس باید متن را به بخش‌های کوچکتر برش دهیم.\n",
    "\n",
    "۲. افزایش دادهٔ آموزشی  \n",
    "با برش‌های همپوشان (مثلاً با جابجایی یک واژه در هر برش)، می‌توان از یک متن، هزاران نمونهٔ آموزشی ساخت.\n",
    "\n",
    "۳. ساخت هدف پیش‌بینی توکن بعدی  \n",
    "برای آموزش مدل‌های زبانی، ورودی یک بخش از متن است و خروجی، همان متن با یک واژه جابه‌جا شده است:\n",
    "ورودی  ← [تو، مرد، خوبی، هستی]  \n",
    "خروجی ← [مرد، خوبی، هستی، ؟]\n",
    "\n",
    "۴. تنوع در یادگیری و جلوگیری از بیش‌برازش (Overfitting)  \n",
    "مدل باید از نمونه‌های گوناگون یاد بگیرد. این تنوع با برش‌های متعدد و تصادفی حاصل می‌شود.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 اگر متن کامل را یک‌جا وارد کنیم چه می‌شود؟\n",
    "\n",
    "- ممکن است مدل کار کند، اما:\n",
    "  - یادگیری بسیار کند و پرخطا خواهد بود.\n",
    "  - امکان آموزش گروهی (Batch) را از دست می‌دهیم.\n",
    "  - استفاده از حافظه به‌شدت زیاد می‌شود.\n",
    "  - مدل احتمالاً به‌خوبی یاد نمی‌گیرد که «توکن بعدی» را پیش‌بینی کند.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 نتیجه‌گیری\n",
    "\n",
    "حتی وقتی تمام شناسه‌های واژه‌ها را در اختیار داریم، برای آموزش مؤثر، همچنان به تکنیک «برش پنجره‌ای» نیاز داریم. این کار کمک می‌کند تا مدل:\n",
    "\n",
    "- بهتر یاد بگیرد،  \n",
    "- با حافظهٔ محدود سازگار باشد،  \n",
    "- و با حجم دادهٔ بیشتر آموزش ببیند.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
